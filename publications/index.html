<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/83a1ddabe21917ec.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-af78f5c2c11af041.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-56ea80454b1da280.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-41db43831ab81c7f.js" async=""></script><script src="/_next/static/chunks/82-0a710f0d74b82ba6.js" async=""></script><script src="/_next/static/chunks/748-55ca435efe938ca9.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-9d9824f6c81ae731.js" async=""></script><link rel="icon" href="/favicon.png" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Publications | Guang Yang</title><meta name="author" content="Guang Yang"/><meta name="keywords" content="Guang Yang,PhD,Research,Zhejiang University"/><meta name="creator" content="Guang Yang"/><meta name="publisher" content="Guang Yang"/><meta property="og:title" content="Guang Yang"/><meta property="og:description" content="Postdoctoral Researcher at Zhejiang University."/><meta property="og:site_name" content="Guang Yang&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Guang Yang"/><meta name="twitter:description" content="Postdoctoral Researcher at Zhejiang University."/><link rel="icon" href="/favicon.png"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Guang Yang</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/"><span class="relative z-10">About</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/publications/"><span class="relative z-10">Publications</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R5pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12"><div style="opacity:0;transform:translateY(20px)"><div class="mb-8"><h1 class="text-4xl font-serif font-bold text-primary mb-4">Publications</h1></div><div class="mb-8 space-y-4"><div class="flex flex-col sm:flex-row gap-4"><div class="relative flex-grow"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="absolute left-3 top-1/2 transform -translate-y-1/2 h-5 w-5 text-neutral-400"><path stroke-linecap="round" stroke-linejoin="round" d="m21 21-5.197-5.197m0 0A7.5 7.5 0 1 0 5.196 5.196a7.5 7.5 0 0 0 10.607 10.607Z"></path></svg><input type="text" placeholder="Search publications..." class="w-full pl-10 pr-4 py-2 rounded-lg border border-neutral-200 dark:border-neutral-800 bg-white dark:bg-neutral-900 focus:ring-2 focus:ring-accent focus:border-transparent transition-all duration-200" value=""/></div><button class="flex items-center justify-center px-4 py-2 rounded-lg border transition-all duration-200 bg-accent text-white border-accent"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 mr-2"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3c2.755 0 5.455.232 8.083.678.533.09.917.556.917 1.096v1.044a2.25 2.25 0 0 1-.659 1.591l-5.432 5.432a2.25 2.25 0 0 0-.659 1.591v2.927a2.25 2.25 0 0 1-1.244 2.013L9.75 21v-6.568a2.25 2.25 0 0 0-.659-1.591L3.659 7.409A2.25 2.25 0 0 1 3 5.818V4.774c0-.54.384-1.006.917-1.096A48.32 48.32 0 0 1 12 3Z"></path></svg>Filters</button></div><div class="overflow-hidden" style="opacity:0;height:0px"><div class="p-4 bg-neutral-50 dark:bg-neutral-800/50 rounded-lg border border-neutral-200 dark:border-neutral-800 flex flex-wrap gap-6"><div class="space-y-2"><label class="text-sm font-medium text-neutral-700 dark:text-neutral-300 flex items-center"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M6.75 3v2.25M17.25 3v2.25M3 18.75V7.5a2.25 2.25 0 0 1 2.25-2.25h13.5A2.25 2.25 0 0 1 21 7.5v11.25m-18 0A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75m-18 0v-7.5A2.25 2.25 0 0 1 5.25 9h13.5A2.25 2.25 0 0 1 21 11.25v7.5"></path></svg> Year</label><div class="flex flex-wrap gap-2"><button class="px-3 py-1 text-xs rounded-full transition-colors bg-accent text-white">All</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">2025</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">2024</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">2023</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">2022</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">2021</button></div></div><div class="space-y-2"><label class="text-sm font-medium text-neutral-700 dark:text-neutral-300 flex items-center"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg> Type</label><div class="flex flex-wrap gap-2"><button class="px-3 py-1 text-xs rounded-full transition-colors bg-accent text-white">All</button><button class="px-3 py-1 text-xs rounded-full capitalize transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">conference</button><button class="px-3 py-1 text-xs rounded-full capitalize transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">journal</button></div></div><div class="space-y-2"><label class="text-sm font-medium text-neutral-700 dark:text-neutral-300 flex items-center"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg> Tags</label><div class="flex flex-wrap gap-2"><button class="px-3 py-1 text-xs rounded-full transition-colors bg-accent text-white">All</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">CCF-A</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">CCF-B</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">CCF-C</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">EI</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">Preprint</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">SCI-Q1</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">SCI-Q2</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">ä¸­æ–‡CCF-A</button><button class="px-3 py-1 text-xs rounded-full transition-colors bg-white dark:bg-neutral-800 text-neutral-600 hover:bg-neutral-100 dark:hover:bg-neutral-700">åŒ—æ ¸</button></div></div></div></div></div><div class="space-y-6"><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J22]</span>Resource-efficient automatic software vulnerability assessment via knowledge distillation and particle swarm optimization</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Chaoyang Gao</span>, </span><span><span class="">Xiang Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Jiyu Wang</span>, </span><span><span class="">Jibin Wang</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Engineering Applications of Artificial Intelligence<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">EAAI&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A resource-efficient framework integrating particle swarm optimization and knowledge distillation.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1016/j.engappai.2025.111914" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/judeomg/PSO-KDVA" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q1</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-C</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J21]</span>Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Wei Zheng</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Dong Liang</span>, </span><span><span class="">Peng Hu</span>, </span><span><span class="">Yukui Yang</span>, </span><span><span class="">Shaohua Peng</span>, </span><span><span class="">Zhenghan Li</span>, </span><span><span class="">Jiahui Feng</span>, </span><span><span class="">Xiao Wei</span>, </span><span><span class="">Kexin Sun</span>, </span><span><span class="">Deyuan Ma</span>, </span><span><span class="">Haotian Cheng</span>, </span><span><span class="">Yiheng Shen</span>, </span><span><span class="">Xing Hu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Terry Yue Zhuo</span>, </span><span><span class="">David Lo</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Preprints<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">Preprints&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A comprehensive literature review of LLMs for Verilog code generation, highlighting their strengths, limitations, and potential applications.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.20944/preprints202511.0656.v2" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>Preprint</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C18]</span>Evaluating and Improving Framework-based Parallel Code Completion with Large Language Models</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Ke Liu</span>, </span><span><span class="">Qinglin Wang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">YiGui Feng</span>, </span><span><span class="">Gencheng Liu</span>, </span><span><span class="">Jie Liu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">ASE&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel framework-based parallel code completion method for code generation, balancing accuracy, efficiency and explainability.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-A</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C17]</span>Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Wei Zheng</span>, </span><span><span class="">Xing Hu</span>, </span><span><span class="">Xin Zhou</span>, </span><span><span class="">David Lo</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">ASE&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel LLM-as-Judge method for code evaluation, balancing accuracy, efficiency and explainability.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/CODE-DITING/CODE-DITING" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-A</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C16]</span>SE-Jury: An LLM-as-Ensemble-Judge Metric for Narrowing the Gap with Human Evaluation in SE</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Xin Zhou</span>, </span><span><span class="">Kisub Kim</span>, </span><span><span class="">Ting Zhang</span>, </span><span><span class="">Martin Weyssow</span>, </span><span><span class="">Luis F. Gomes</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Kui Liu</span>, </span><span><span class="">Xin Xia</span>, </span><span><span class="">David Lo</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">ASE&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel LLM-as-Ensemble-Judge metric for SE, balancing accuracy, efficiency and explainability.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/Xin-Zhou-smu/LLMJudge" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-A</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J20]</span>The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Wei Zheng</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Yifan Sun</span>, </span><span><span class="">Fengji Zhang</span>, </span><span><span class="">Terry Yue Zhuo</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>arXiv<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">arXiv&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel reranking method for Verilog code generation, highlighting their strengths, limitations, and potential applications.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.48550/arXiv.2509.20215" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>Preprint</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C15]</span>Beyond Sequences: Two-dimensional Representation and Dependency Encoding for Code Generation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Wei Cheng</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">ACL&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel dependency encoding approach for code generation, highlighting its generalizability, context understanding and retrieval, as well as interpretability in code generation.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.18653/v1/2025.acl-long.308" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-A</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J19]</span>Anchor Attention, Small Cache: Code Generation With Large Language Models</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Harald C. Gall</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>IEEE Transactions on Software Engineering<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">TSE&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel attention mechanism and cache mechanism for code generation with LLMs, achieving significant reduction in KV cache requirements while preserving the majority of model&#x27;s performance.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/TSE.2025.3570680" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NUAAZXY/Anchor_Coder" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q1</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-A</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J18]</span>Less is More: DocString Compression in Code Generation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Wei Cheng</span>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Terry Yue Zhuo</span>, </span><span><span class="">Ke Liu</span>, </span><span><span class="">Xin Zhou</span>, </span><span><span class="">David Lo</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>ACM Transactions on Software Engineering and Methodology<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">TOSEM&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel DocString compression method for code generation, achieving significant reduction in token processing cost while preserving the quality of the generated code.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1145/3735636" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/ShortenDoc" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q1</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-A</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J17]</span>Assessing and improving syntactic adversarial robustness of pre-trained models for code translation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Tingting Han</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Information and Software Technology<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">IST&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel approach to assess and improve the syntactic adversarial robustness of PTMs in code translation, achieving significant reduction in syntactic adversarial robustness while preserving the majority of model&#x27;s performance.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/https://doi.org/10.1016/j.infsof.2025.107699" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/CoTR" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q2</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J16]</span>Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Terry Zhuo</span>, </span><span><span class="">David Lo</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>ACM Transactions on Software Engineering and Methodology<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">TOSEM&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A general and effective loss function DeCE (Deceptive Cross-Entropy) to defend Code Language Models against backdoor attacks, preventing overfitting to backdoor triggers.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1145/3728639" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/DeCE" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q1</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-A</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J15]</span>Less is More: Towards Green Code Large Language Models via Unified Structural Pruning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Wei Cheng</span>, </span><span><span class="">Ke Liu</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Terry Yue Zhuo</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>arXiv<!-- --> <!-- -->2025</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">arXiv&#x27;25</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel unified structural pruning method for Code Large Language Models, achieving significant reduction in computational demands and energy consumption while preserving the majority of model&#x27;s performance.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.48550/arXiv.2412.15921" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/Flab-Pruner/Flab-Pruner" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>Preprint</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C14]</span>RegexExplainer: Automatic Description Generation for Regular Expressions via Transformer</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yun Wu</span>, </span><span><span class="">Shuangbo Cao</span>, </span><span><span class="">Tianyue Liu</span>, </span><span><span class="">Xiao Yang</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 4th International Conference on Communication Technology and Information Technology<!-- --> <!-- -->2024</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">ICCTIT&#x27;24</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel method to automatically generate functional descriptions for regular expressions, achieving significant improvement in performance compared to the state-of-the-art baselines.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/ICCTIT64404.2024.10928415" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J14]</span>Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Terry Yue Zhuo</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>IEEE Transactions on Software Engineering<!-- --> <!-- -->2024</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">TSE&#x27;24</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel approach COTTON to automatically generate CoTs for code generation, achieving significant improvement in performance compared to the state-of-the-art baselines.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/TSE.2024.3440503" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/COTTON" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q1</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-A</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J13]</span>Context-aware code generation with synchronous bidirectional decoder</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Yu Zhou</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Tingting Han</span>, </span><span><span class="">Taolue Chen</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Journal of Systems and Software<!-- --> <!-- -->2024</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">JSS&#x27;24</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel context-sensitive model employing a bidirectional decoder to generate tokens in two different orders synchronously and interactively.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/https://doi.org/10.1016/j.jss.2024.112066" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q2</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J12]</span>Automatic bi-modal question title generation for Stack Overflow with prompt learning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Shaoyu Yang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Ke Liu</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Chi Yu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Empirical Software Engineering<!-- --> <!-- -->2024</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">EMSE&#x27;24</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel approach SOTitle+ to automatically generate the titles for Stack Overflow question posts.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1007/s10664-024-10466-4" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/shaoyuyoung/SOTitlePlus" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q2</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J11]</span>Automatic smart contract comment generation via large language models and in-context learning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Junjie Zhao</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yiheng Shen</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Information and Software Technology<!-- --> <!-- -->2024</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">IST&#x27;24</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel approach SCCLLM to automatically generate the comments for smart contract code.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1016/j.infsof.2024.107405" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/jun-jie-zhao/SCCLLM" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q2</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J10]</span>Bash comment generation via data augmentation and semantic-aware CodeBERT</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yiheng Shen</span>, </span><span><span class="">Xiaolin Ju</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Automated Software Engineering<!-- --> <!-- -->2024</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">ASEJ&#x27;24</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel two-module method named Bash2Com for Bash code comments generation.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1007/s10515-024-00431-2" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/syhstudy/Bash2Com" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q2</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J9]</span>How important are good method names in neural code generation? a model robustness perspective</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span>, </span><span><span class="">Wenhua Yang</span>, </span><span><span class="">Tao Yue</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Taolue Chen</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>ACM Transactions on Software Engineering and Methodology<!-- --> <!-- -->2024</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">TOSEM&#x27;24</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel approach RADAR to enhance the performance of PCGMs from a model robustness perspective.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1145/3630010" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/RADAR" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q1</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-A</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J8]</span>CodeScore-Rï¼šç”¨äºŽè¯„ä¼°ä»£ç åˆæˆåŠŸèƒ½å‡†ç¡®æ€§çš„è‡ªåŠ¨åŒ–é²æ£’æŒ‡æ ‡</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Yang Guang</span>, </span><span><span class="">Zhou Yu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Chen Xiang</span>, </span><span><span class="">Zhang Xiangyu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>è®¡ç®—æœºç ”ç©¶ä¸Žå‘å±•<!-- --> <!-- -->2024</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">è®¡ç®—æœºç ”ç©¶ä¸Žå‘å±•&#x27;24</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel automated robust metric CodeScore-R for evaluating the functional accuracy of code synthesis, achieving significant improvement in performance compared to the state-of-the-art baselines.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.7544/issn1000-1239.202330715" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>ä¸­æ–‡CCF-A</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>åŒ—æ ¸</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C13]</span>Syntax-Aware Retrieval Augmented Code Generation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Yu Zhou</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Taolue Chen</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Findings of the Association for Computational Linguistics: EMNLP<!-- --> <!-- -->2023</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">EMNLP&#x27;23</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel token-level retrieval augmented code generation method $k$NN-TRANX.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.18653/v1/2023.findings-emnlp.90" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NUAAZXY/kNN-TRANX" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J7]</span>A syntax-guided multi-task learning approach for Turducken-style code generation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Yiran Xu</span>, </span><span><span class="">Tingting Han</span>, </span><span><span class="">Taolue Chen</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Empirical Software Engineering<!-- --> <!-- -->2023</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">EMSE&#x27;23</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel syntax-guided multi-task learning approach for Turducken-style code generation.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1007/s10664-023-10372-1" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/TurduckenGen" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q2</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C12]</span>EDP-BGCNN: Effective Defect Prediction via BERT-based Graph Convolutional Neural Network</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Hao Shen</span>, </span><span><span class="">Xiaolin Ju</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 47th Annual Computers, Software, and Applications Conference<!-- --> <!-- -->2023</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">COMPSAC&#x27;23</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel BERT-based Graph Convolutional Neural Network for effective defect prediction.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/COMPSAC57700.2023.00114" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-C</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C11]</span>An Empirical Study of Adversarial Training in Code Comment Generation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yiheng Shen</span>, </span><span><span class="">Xiaolin Ju</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 35th International Conference on Software Engineering and Knowledge Engineering<!-- --> <!-- -->2023</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">SEKE&#x27;23</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">An empirical study of adversarial training in code comment generation.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.18293/SEKE2023-108" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-C</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C10]</span>CCGRA: Smart Contract Code Comment Generation with Retrieval-enhanced Approach</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Shizhan Chen Zhenhua Zhang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 35th International Conference on Software Engineering and Knowledge Engineering<!-- --> <!-- -->2023</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">SEKE&#x27;23</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel retrieval-enhanced approach CCGRA that leverages retrieval knowledge to generate high-quality comments for Solidity language code.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.18293/SEKE2023-090" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/ZZHbible/CCGRA" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-C</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J6]</span>åŸºäºŽåŒé‡ä¿¡æ¯æ£€ç´¢çš„Bashä»£ç æ³¨é‡Šç”Ÿæˆæ–¹æ³•</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Chen Xiang</span>, </span><span><span class="">Chi Yu</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Xuelian Pu</span>, </span><span><span class="">Zhanqi Cui</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>è½¯ä»¶å­¦æŠ¥<!-- --> <!-- -->2023</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">è½¯ä»¶å­¦æŠ¥&#x27;23</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel approach to automatically generate the comments for Bash code based on dual information retrieval.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.13328/j.cnki.jos.006690" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/ExplainBash/explainbash" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>ä¸­æ–‡CCF-A</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>åŒ—æ ¸</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J5]</span>ExploitGen: Template-augmented exploit code generation based on CodeBERT</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Tingting Han</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">ðŸ’Œ</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Journal of Systems and Software<!-- --> <!-- -->2023</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">JSS&#x27;23</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel template-augmented exploit code generation approach ExploitGen based on CodeBERT.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/https://doi.org/10.1016/j.jss.2022.111577" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/ExploitGen" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q2</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C9]</span>BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Chi Yu</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Ke Liu</span>, </span><span><span class="">Yanlin Zhou</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 2022 IEEE International Conference on Software Maintenance and Evolution<!-- --> <!-- -->2022</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">ICSME&#x27;22</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel retrieval-augmented Bash code comment generation method BASHEXPLAINER based on fine-tuned CodeBERT.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/ICSME55016.2022.00016" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/BashExplainer" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C8]</span>BUG-T5: A Transformer-based Automatic Title Generation Method for Bug Reports</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Xinyi Tian</span>, </span><span><span class="">Jingkun Wu</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 3rd International Conference on Big Data &amp; Artificial Intelligence &amp; Software Engineering<!-- --> <!-- -->2022</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">ICBASE&#x27;22</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel Transformer-based automatic title generation method for bug reports.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C7]</span>EL-CodeBert: Better Exploiting CodeBert to Support Source Code-Related Classification Tasks</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Ke Liu</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Yanlin Zhou</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 13th Asia-Pacific Symposium on Internetware<!-- --> <!-- -->2022</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">Internetware&#x27;22</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel approach EL-CodeBert to better exploit CodeBert for source code-related classification tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1145/3545258.3545260" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/EL-CodeBert" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-C</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C6]</span>SOTitle: A Transformer-based Post Title Generation Approach for Stack Overflow</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Ke Liu</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Chi Yu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>2022 IEEE International Conference on Software Analysis, Evolution and Reengineering<!-- --> <!-- -->2022</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">SANER&#x27;22</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel Transformer-based post title generation approach SOTitle for Stack Overflow.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/SANER53432.2022.00075" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/SOTitle" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C5]</span>DualSC: Automatic Generation and Summarization of Shellcode via Transformer and Dual Learning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Yanlin Zhou</span>, </span><span><span class="">Chi Yu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>2022 IEEE International Conference on Software Analysis, Evolution and Reengineering<!-- --> <!-- -->2022</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">SANER&#x27;22</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel approach DualSC to solve the automatic shellcode generation and summarization tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/SANER53432.2022.00052" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/DualSC" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-B</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J4]</span>CCGIR: Information retrieval-based code comment generation method for smart contracts</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Ke Liu</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Yanlin Zhou</span>, </span><span><span class="">Chi Yu</span>, </span><span><span class="">Hao Lin</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Knowledge-Based Systems<!-- --> <!-- -->2022</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">KBS&#x27;22</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel information retrieval-based code comment generation method CCGIR for smart contracts.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/https://doi.org/10.1016/j.knosys.2021.107858" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/CCGIR" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>SCI-Q1</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-C</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C4]</span>Fine-grained Pseudo-code Generation Method via Code Feature Extraction and Transformer</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yanlin Zhou</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Chi Yu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 28th Asia-Pacific Software Engineering Conference<!-- --> <!-- -->2021</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">APSEC&#x27;21</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel deep pseudo-code generation method DeepPseudo via code feature extraction and Transformer.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/APSEC53868.2021.00029" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/DeepPseudo" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-C</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C3]</span>EKD-BSP: Bug Report Severity Prediction by Extracting Keywords from Description</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yanxin Jia</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Shuyuan Xu</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Jinxin Cao</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 8th International Conference on Dependable Systems and Their Applications<!-- --> <!-- -->2021</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">DSA&#x27;21</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel method EKD-BSP to predict the severity of bug reports by extracting keywords from the description.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/DSA52907.2021.00014" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C2]</span>ComFormer: Code Comment Generation via Transformer and Fusion Method-based Hybrid Code Representation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Jinxin Cao</span>, </span><span><span class="">Shuyuan Xu</span>, </span><span><span class="">Zhanqi Cui</span>, </span><span><span class="">Chi Yu</span>, </span><span><span class="">Ke Liu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 8th International Conference on Dependable Systems and Their Applications<!-- --> <!-- -->2021</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">DSA&#x27;21</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel method ComFormer to generate code comments via Transformer and fusion method-based hybrid code representation.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/DSA52907.2021.00013" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/ComFormer" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[C1]</span>DeepSCC: Source Code Classification Based on Fine-Tuned RoBERTa (S)</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yanlin Zhou</span>, </span><span><span class="">Chi Yu</span>, </span><span><span class="">Xiang Chen</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>Proceedings of the 33rd International Conference on Software Engineering and Knowledge Engineering<!-- --> <!-- -->2021</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">SEKE&#x27;21</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel source code classification method DeepSCC based on fine-tuned RoBERTa.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.18293/SEKE2021-005" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/NTDXYG/DeepSCC" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>CCF-C</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J3]</span>åŸºäºŽæ·±åº¦å­¦ä¹ çš„ Stack Overflow é—®é¢˜å¸–åˆ†ç±»æ–¹æ³•</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yanxin Jia</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Shuyuan Xu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>å‰æž—å¤§å­¦å­¦æŠ¥ (ç†å­¦ç‰ˆ)<!-- --> <!-- -->2021</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">å‰æž—å¤§å­¦å­¦æŠ¥ (ç†å­¦ç‰ˆ)&#x27;21</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A deep learning-based classification methods for question post on Stack Overflow.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.13413/j.cnki.jdxblxb.2020165" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>åŒ—æ ¸</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J2]</span>ORESP:åŸºäºŽæœ‰åºå›žå½’çš„è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦é¢„æµ‹æ–¹æ³•</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yanxin Jia</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Hao Lu</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Hao Lin</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>è®¡ç®—æœºåº”ç”¨ç ”ç©¶<!-- --> <!-- -->2021</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">è®¡ç®—æœºåº”ç”¨ç ”ç©¶&#x27;21</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A novel method ORESP to predict the severity of software defects based on ordered regression.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.19734/j.issn.1001-3695.2020.07.0249" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>åŒ—æ ¸</span></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><span class="mb-2 text-primary">[J1]</span>ä»£ç æ³¨é‡Šè‡ªåŠ¨ç”Ÿæˆæ–¹æ³•ç»¼è¿°</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Xiang Chen</span>, </span><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Zhanqi Cui</span>, </span><span><span class="">Guozhu Meng</span>, </span><span><span class="">Zan Wang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3 flex items-center justify-between"><span>è½¯ä»¶å­¦æŠ¥<!-- --> <!-- -->2021</span><span class="inline-block px-1.5 py-0.5 text-xs rounded bg-accent/10 text-accent font-bold border border-accent/20">è½¯ä»¶å­¦æŠ¥&#x27;21</span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A review of automatic code comment generation methods.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.13328/j.cnki.jos.006258" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button><div class="flex-grow"></div><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>EI</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>ä¸­æ–‡CCF-A</span><span class="inline-flex items-center px-2.5 py-1 rounded-md text-xs font-medium bg-red-50 dark:bg-red-900/30 text-red-700 dark:text-red-300 border border-red-100 dark:border-red-800"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1"><path stroke-linecap="round" stroke-linejoin="round" d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z"></path></svg>åŒ—æ ¸</span></div></div></div></div></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->November 18, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ðŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-af78f5c2c11af041.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-41db43831ab81c7f.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-41db43831ab81c7f.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-41db43831ab81c7f.js\"],\"default\"]\n8:I[9665,[],\"MetadataBoundary\"]\na:I[9665,[],\"OutletBoundary\"]\nd:I[4911,[],\"AsyncMetadataOutlet\"]\nf:I[9665,[],\"ViewportBoundary\"]\n11:I[6614,[],\"\"]\n:HL[\"/_next/static/css/83a1ddabe21917ec.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"8VsTx3EH55hl3zRkJOFAh\",\"p\":\"\",\"c\":[\"\",\"publications\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"publications\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/83a1ddabe21917ec.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.png\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"}],\"siteTitle\":\"Guang Yang\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"November 18, 2025\"}]]}]}]]}]]}],{\"children\":[[\"slug\",\"publications\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"cIQz2vNMJ5hbtrijWa2ZL\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[4911,[],\"AsyncMetadata\"]\n15:I[6669,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"82\",\"static/chunks/82-0a710f0d74b82ba6.js\",\"748\",\"static/chunks/748-55ca435efe938ca9.js\",\"182\",\"static/chunks/app/%5Bslug%5D/page-9d9824f6c81ae731.js\"],\"default\"]\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n16:T5d7,The increasing complexity of software systems has led to a surge in cybersecurity vulnerabilities, necessitating efficient and scalable solutions for vulnerability assessment. However, the deployment of large pre-trained models in real-world scenarios is hindered by their substantial computational and storage demands. To address this challenge, we propose a novel resource-efficient framework that integrates knowledge distillation and particle swarm optimization to enable automated vulnerability assessment. Our framework employs a two-stage approach: First, particle swarm optimization is utilized to optimize the architecture of a compact student model, balancing computational efficiency and model capacity. Second, knowledge distillation is applied to transfer critical vulnerability assessment knowledge from a large teacher model to the optimized student model. This process significantly reduces the model size while maintaining high performance. Experimental results on an enhanced MegaVul dataset, comprising 12,071 CVSS (Common Vulnerability Scoring System) v3 annotated vulnerabilities, demonstrate the effectiveness of our approach. Our approach achieves a 99.4% reduction in model size while retaining 89.3% of the original modelâ€™s accuracy. Furthermore, it outperforms state-of-the-art baselines by 1.7% in accuracy with 60% fewer parameters. The framework also reduces training time by 72.1% and architecture search time by 34.88% compared to traditional genetic algorithms.17:T7e0,@article{gao2025resource,\n  abbr = {EAAI'25},\n  title = {Resource-efficient automatic software vulnerability assessment via knowledge distillation and particle swarm optimization},\n"])</script><script>self.__next_f.push([1,"  author = {Gao, Chaoyang and Chen, Xiang and Wang, Jiyu and Wang, Jibin and Yang, Guang},\n  journal = {Engineering Applications of Artificial Intelligence},\n  volume = {160},\n  pages = {111914},\n  year = {2025},\n  month = {11},\n  publisher = {Elsevier},\n  doi = {10.1016/j.engappai.2025.111914},\n  abstract = {The increasing complexity of software systems has led to a surge in cybersecurity vulnerabilities, necessitating efficient and scalable solutions for vulnerability assessment. However, the deployment of large pre-trained models in real-world scenarios is hindered by their substantial computational and storage demands. To address this challenge, we propose a novel resource-efficient framework that integrates knowledge distillation and particle swarm optimization to enable automated vulnerability assessment. Our framework employs a two-stage approach: First, particle swarm optimization is utilized to optimize the architecture of a compact student model, balancing computational efficiency and model capacity. Second, knowledge distillation is applied to transfer critical vulnerability assessment knowledge from a large teacher model to the optimized student model. This process significantly reduces the model size while maintaining high performance. Experimental results on an enhanced MegaVul dataset, comprising 12,071 CVSS (Common Vulnerability Scoring System) v3 annotated vulnerabilities, demonstrate the effectiveness of our approach. Our approach achieves a 99.4% reduction in model size while retaining 89.3% of the original modelâ€™s accuracy. Furthermore, it outperforms state-of-the-art baselines by 1.7% in accuracy with 60% fewer parameters. The framework also reduces training time by 72.1% and architecture search time by 34.88% compared to traditional genetic algorithms.},\n  tags = {SCI-Q1; CCF-C}\n}18:T4b9,Code generation represents a critical intersection of Software Engineering (SE) and Artificial Intelligence (AI). Within this broader landscape, Verilog, as a representative hardware description language ("])</script><script>self.__next_f.push([1,"HDL), is fundamental to Electronic Design Automation (EDA), recent research has increasingly focused on leveraging Large Language Models (LLMs) to automate Verilog code generation, particularly at the Register Transfer Level (RTL) design. Despite growing interest, a comprehensive survey of this domain remains absent. This review fill addresses this gap by providing a systematic literature review of LLM-based Verilog code generation, analyzing 102 papers (70 published and 32 high-quality preprints) from SE, AI, and EDA venues. We structure our analysis around four key research questions: (1) identifying the LLMs utilized, (2) examining evaluation datasets and metrics, (3) categorizing generation techniques, and (4) analyzing alignment approaches. Furthermore, we synthesize findings to identify critical limitations in current studies regarding effectiveness and integration. Finally, we outline a roadmap highlighting potential opportunities for future research in LLM-assisted hardware design.19:T6fd,@article{yang2025large,\n  title = {Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead},\n  abbr = {Preprints'25},\n  author = {Guang Yang and Wei Zheng and Xiang Chen and Dong Liang and Peng Hu and Yukui Yang and Shaohua Peng and Zhenghan Li and Jiahui Feng and Xiao Wei and Kexin Sun and Deyuan Ma and Haotian Cheng and Yiheng Shen and Xing Hu and Terry Yue Zhuo and David Lo},\n  year = {2025},\n  month = {11},\n  journal = {Preprints},\n  publisher = {Preprints},\n  tags = {Preprint},\n  doi = {10.20944/preprints202511.0656.v2},\n  abstract = {Code generation represents a critical intersection of Software Engineering (SE) and Artificial Intelligence (AI). Within this broader landscape, Verilog, as a representative hardware description language (HDL), is fundamental to Electronic Design Automation (EDA), recent research has increasingly focused on leveraging Large Language Models (LLMs) to automate Verilog code generation, particularly at the Register Transfer Level (RTL) design. Despite growing"])</script><script>self.__next_f.push([1," interest, a comprehensive survey of this domain remains absent. This review fill addresses this gap by providing a systematic literature review of LLM-based Verilog code generation, analyzing 102 papers (70 published and 32 high-quality preprints) from SE, AI, and EDA venues. We structure our analysis around four key research questions: (1) identifying the LLMs utilized, (2) examining evaluation datasets and metrics, (3) categorizing generation techniques, and (4) analyzing alignment approaches. Furthermore, we synthesize findings to identify critical limitations in current studies regarding effectiveness and integration. Finally, we outline a roadmap highlighting potential opportunities for future research in LLM-assisted hardware design.}\n}1a:T6c4,Modern computing architectures (e.g., multi-core CPUs, GPUs, distributed systems) rely on parallel code implemented via frameworks such as OpenMP, MPI, and CUDA. While large language models (LLMs) have shown strong performance in general code generation, they struggle with the structured reasoning required for parallel programming, such as handling concurrency, synchronization, and framework-specific semantics. In practical parallel code development, a common workflow begins with sequential code and incrementally introduces parallel directive codes. We formalize this process as the task of textbfframework-based parallel code completion (FPCC), which involves three subtasks: identifying insertion points, selecting parallel frameworks, and completing parallel directive codes. To support this task, we construct a high-quality dataset of 16,615 framework-based parallel code pairs across six widely used frameworks, labeled with directive points, parallel frameworks, and the code of parallel directives. Empirical results show that six popular LLMs perform poorly on FPCC, particularly struggling with identifying insertion points and completing correct directive codes. To address these limitations, we propose HPCL, a curriculum-based fine-tuning framework that progressively "])</script><script>self.__next_f.push([1,"improves model capabilities in insertion point identification, parallel framework selection, and parallel directive code completion. Our approach achieves substantial improvements, yielding an 17.82% increase in EM and a 5.43% improvement in DIR scores over LLM-based baselines. Finally, expert-guided error analysis reveals common failure patterns and suggests future directions in retrieval-augmented completion and consistency-aware training.1b:T88c,"])</script><script>self.__next_f.push([1,"@inproceedings{liu2025evaluating,\n  title = {Evaluating and Improving Framework-based Parallel Code Completion with Large Language Models},\n  abbr = {ASE'25},\n  author = {Ke Liu and Qinglin Wang and Xiang Chen and Guang Yang and YiGui Feng and Gencheng Liu and Jie Liu},\n  tags = {CCF-A;EI},\n  booktitle = {Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering},\n  year = {2025},\n  month = {11},\n  abstract = {Modern computing architectures (e.g., multi-core CPUs, GPUs, distributed systems) rely on parallel code implemented via frameworks such as OpenMP, MPI, and CUDA. While large language models (LLMs) have shown strong performance in general code generation, they struggle with the structured reasoning required for parallel programming, such as handling concurrency, synchronization, and framework-specific semantics. In practical parallel code development, a common workflow begins with sequential code and incrementally introduces parallel directive codes. We formalize this process as the task of \\textbf{framework-based parallel code completion} (FPCC), which involves three subtasks: identifying insertion points, selecting parallel frameworks, and completing parallel directive codes.\r\nTo support this task, we construct a high-quality dataset of 16,615 framework-based parallel code pairs across six widely used frameworks, labeled with directive points, parallel frameworks, and the code of parallel directives. Empirical results show that six popular LLMs perform poorly on FPCC, particularly struggling with identifying insertion points and completing correct directive codes.\r\nTo address these limitations, we propose HPCL, a curriculum-based fine-tuning framework that progressively improves model capabilities in insertion point identification, parallel framework selection, and parallel directive code completion. Our approach achieves substantial improvements, yielding an 17.82% increase in EM and a 5.43% improvement in DIR scores over LLM-based baselines. Finally, expert-guided error analysis reveals common failure patterns and suggests future directions in retrieval-augmented completion and consistency-aware training.}\n}"])</script><script>self.__next_f.push([1,"1c:T736,Trustworthy evaluation methods for code snippets play a crucial role in neural code generation. Traditional methods, which either rely on reference solutions or require executable test cases, have inherent limitation in flexibility and scalability. The recent LLM-as-Judge methodology offers a promising alternative by directly evaluating functional consistency between the problem description and the generated code. To systematically understand the landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical study across three diverse datasets. Our investigation reveals the pros and cons of two categories of LLM-as-Judge methods: the methods based on general foundation models can achieve good performance but require complex prompts and lack explainability, while the methods based on reasoning foundation models provide better explainability with simpler prompts but demand substantial computational resources due to their large parameter sizes. To address these limitations, we propose CODE-DITING, a novel code evaluation method that balances accuracy, efficiency and explainability. We develop a data distillation framework that effectively transfers reasoning capabilities from DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing evaluation explainability and reducing the computational cost. With the majority vote strategy in the inference process, CODE-DITING 1.5B outperforms all models with the same magnitude of parameters and achieves performance which would normally exhibit in a model with 5 times of parameter scale. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the parameter volume of these large models. Further experiments show that CODEDITING is robust to preference leakage and can serve as a promising alternative for code evaluation.1d:T903,"])</script><script>self.__next_f.push([1,"@inproceedings{yang2025codediting,\n  title = {Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases},\n  abbr = {ASE'25},\n  author = {Yang, Guang and Zhou, Yu and Chen, Xiang and Zheng, Wei and Hu, Xing and Zhou, Xin and Lo, David and Chen, Taolue},\n  tags = {CCF-A;EI},\n  booktitle = {Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering},\n  year = {2025},\n  month = {11},\n  abstract = {Trustworthy evaluation methods for code snippets play a crucial role in neural code generation. Traditional methods, which either rely on reference solutions or require executable test cases, have inherent limitation in flexibility and scalability. The recent LLM-as-Judge methodology offers a promising alternative by directly evaluating functional consistency between the problem description and the generated code. To systematically understand the landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical study across three diverse datasets. Our investigation reveals the pros and cons of two categories of LLM-as-Judge methods: the methods based on general foundation models can achieve good performance but require complex prompts and lack explainability, while the methods based on reasoning foundation models provide better explainability with simpler prompts but demand substantial computational resources due to their large parameter sizes. To address these limitations, we propose CODE-DITING, a novel code evaluation method that balances accuracy, efficiency and explainability. We develop a data distillation framework that effectively transfers reasoning capabilities from DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing evaluation explainability and reducing the computational cost. With the majority vote strategy in the inference process, CODE-DITING 1.5B outperforms all models with the same magnitude of parameters and achieves performance which would normally exhibit in a model with 5 times of parameter scale. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the parameter volume of these large models. Further experiments show that CODEDITING is robust to preference leakage and can serve as a promising alternative for code evaluation.}\n}"])</script><script>self.__next_f.push([1,"1e:T6f2,Large Language Models (LLMs) and other automated techniques have been increasingly used to support software developers by generating software artifacts such as code snippets, patches, and comments. However, accurately assessing the correctness of these generated artifacts remains a significant challenge. On one hand, human evaluation provides high accuracy but is labor-intensive and lacks scalability. On the other hand, many automatic evaluation metrics are scalable and require minimal human effort, but they often fail to accurately reflect the actual correctness of generated software artifacts. In this paper, we present SE-Jury, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. SE-Jury first defines five distinct evaluation strategies, each implemented as an independent judge. A dynamic team selection mechanism then identifies the most appropriate subset of judges as a team to produce a final correctness score through ensembling. We evaluate SE-Jury across a diverse set of software engineering (SE) benchmarks, including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess, which span three popular SE tasks: code generation, automated program repair, and code summarization. Experimental results demonstrate that SE-Jury consistently achieves a higher correlation with human judgments, with improvements ranging from 34.4% to 113.0% over existing automatic metrics. Furthermore, SE-Jury reaches agreement levels with human annotators that are close to inter-annotator agreement in code generation and program repair tasks. These findings underscore SE-Juryâ€™s potential as a scalable and reliable alternative to human evaluation in these SE tasks.1f:T8ce,"])</script><script>self.__next_f.push([1,"@inproceedings{zhou2025sejury,\n  title = {SE-Jury: An LLM-as-Ensemble-Judge Metric for Narrowing the Gap with Human Evaluation in SE},\n  abbr = {ASE'25},\n  author = {Xin Zhou and Kisub Kim and Ting Zhang and Martin Weyssow and Luis F. Gomes and Guang Yang and Kui Liu and Xin Xia and David Lo},\n  tags = {CCF-A;EI},\n  booktitle = {Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering},\n  year = {2025},\n  month = {11},\n  abstract = {Large Language Models (LLMs) and other automated techniques have been increasingly used to support software developers by generating software artifacts such as code snippets, patches, and comments. However, accurately assessing the correctness of these generated artifacts remains a significant challenge. On one hand, human evaluation provides high accuracy but is labor-intensive and lacks scalability. On the other hand, many automatic evaluation metrics are scalable and require minimal human effort, but they often fail to accurately reflect the actual correctness of generated software artifacts.\r\nIn this paper, we present SE-Jury, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. SE-Jury first defines five distinct evaluation strategies, each implemented as an independent judge. A dynamic team selection mechanism then identifies the most appropriate subset of judges as a team to produce a final correctness score through ensembling. We evaluate SE-Jury across a diverse set of software engineering (SE) benchmarks, including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess, which span three popular SE tasks: code generation, automated program repair, and code summarization. Experimental results demonstrate that SE-Jury consistently achieves a higher correlation with human judgments, with improvements ranging from 34.4% to 113.0% over existing automatic metrics. Furthermore, SE-Jury reaches agreement levels with human annotators that are close to inter-annotator agreement in code generation and program repair tasks. These findings underscore SE-Juryâ€™s potential as a scalable and reliable alternative to human evaluation in these SE tasks.}\n}"])</script><script>self.__next_f.push([1,"20:T49c,@article{yang2025cream,\n  title = {The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation},\n  abbr = {arXiv'25},\n  author = {Yang, Guang and Zheng, Wei and Chen, Xiang and Sun, Yifan and Zhang, Fengji and Zhuo, Terry Yue},\n  journal = {arXiv},\n  year = {2025},\n  month = {9},\n  tags = {Preprint},\n  doi = {10.48550/arXiv.2509.20215},\n  abstract = {LLMs face significant challenges in Verilog generation due to limited domain-specific knowledge. While sampling techniques improve pass@k metrics, hardware engineers need one trustworthy solution rather than uncertain candidates. To bridge this gap, we formulate it as a semantic alignment problem between requirements and Verilog implementations, and propose VCD-RNK, a discriminator model tailored for efficient Verilog code reranking. Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling expert knowledge across three dimensions: code semantic analysis, test case generation, and functional correctness assessment. By explicitly simulating the above reasoning processes during inference, VCD-RNK effectively avoids computationally intensive test execution in existing methods.}\n}21:T408,The advent of large language models has significantly advanced automatic code generation, transforming the way programmers writing code. Inspired by natural language processing, mainstream code generation approaches represent code as a linear sequence of tokens. In this paper, we propose to represent code snippets as two-dimensional entities, where both code lines and tokens within lines are explicitly modeled. This representation allows us to capture the hierarchical and spatial structure of code, especially the dependencies between code lines. Our method CoDE introduces a dependency encoding approach that leverages dictionary learning to perform semantic matching between code lines. As such, it avoids the reliance on strict position indices, leading to better generalization to code with diverse context and lengths. We thoroughly evaluate "])</script><script>self.__next_f.push([1,"CoDE based on four categories of tasks. The experimental results showcase its generalizability, context understanding and retrieval, as well as interpretability in code generation.22:T5ef,@inproceedings{zhang2025beyond,\n  title = {Beyond Sequences: Two-dimensional Representation and Dependency Encoding for Code Generation},\n  abbr = {ACL'25},\n  author = {Zhang, Xiangyu and Zhou, Yu and Yang, Guang and Cheng, Wei and Chen, Taolue},\n  tags = {CCF-A;EI},\n  doi = {10.18653/v1/2025.acl-long.308},\n  booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics},\n  pages = {6157--6172},\n  year = {2025},\n  month = {7},\n  abstract = {The advent of large language models has significantly advanced automatic code generation, transforming the way programmers writing code. Inspired by natural language processing, mainstream code generation approaches represent code as a linear sequence of tokens. In this paper, we propose to represent code snippets as two-dimensional entities, where both code lines and tokens within lines are explicitly modeled. This representation allows us to capture the hierarchical and spatial structure of code, especially the dependencies between code lines. Our method CoDE introduces a dependency encoding approach that leverages dictionary learning to perform semantic matching between code lines. As such, it avoids the reliance on strict position indices, leading to better generalization to code with diverse context and lengths. We thoroughly evaluate CoDE based on four categories of tasks. The experimental results showcase its generalizability, context understanding and retrieval, as well as interpretability in code generation.}\n}23:T58d,The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism"])</script><script>self.__next_f.push([1," which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model's performance.24:T749,@ARTICLE{11005718,\n  author = {Zhang, Xiangyu and Zhou, Yu and Yang, Guang and Gall, Harald C. and Chen, Taolue},\n  abbr = {TSE'25},\n  tags = {SCI-Q1; CCF-A},\n  journal = {IEEE Transactions on Software Engineering},\n  title = {Anchor Attention, Small Cache: Code Generation With Large Language Models},\n  year = {2025},\n  month = {5},\n  volume = {51},\n  number = {6},\n  pages = {1866-1881},\n  doi = {10.1109/TSE.2025.3570680},\n  abstract = {The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP "])</script><script>self.__next_f.push([1,"often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model's performance.}\n}25:T55a,The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings, which capture user requirements for the code and are typically used as the prompt for LLMs, often contain redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study show that the state-of-the-art prompt compression methods achieve only about 10% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc, dedicated to DocString compression for code generation. Our experiments on six code generation datasets, five open-source LLMs (1B to 10B parameters) and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25â€“40% compression while preserving the quality of generated code, outperfo"])</script><script>self.__next_f.push([1,"rming other baseline methods at similar compression levels. The benefit of this method is to improve efficiency and reduce the token processing cost while maintaining the quality of the generated code, especially when calling third-party APIs.26:T7a9,@article{10.1145/3735636,\n  author = {Yang, Guang and Zhou, Yu and Cheng, Wei and Zhang, Xiangyu and Chen, Xiang and Zhuo, Terry Yue and Liu, Ke and Zhou, Xin and Lo, David and Chen, Taolue},\n  abbr = {TOSEM'25},\n  tags = {SCI-Q1; CCF-A},\n  title = {Less is More: DocString Compression in Code Generation},\n  year = {2025},\n  month = {5},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  issn = {1049-331X},\n  doi = {10.1145/3735636},\n  abstract = {The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings, which capture user requirements for the code and are typically used as the prompt for LLMs, often contain redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study show that the state-of-the-art prompt compression methods achieve only about 10\\% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc, dedicated to DocString compression for code generation. Our experiments on six code generation datasets, five open-source LLMs (1B to 10B parameters) and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25â€“40\\% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this method is to improve efficiency and reduce the token processing cost while maintaining the quality o"])</script><script>self.__next_f.push([1,"f the generated code, especially when calling third-party APIs.},\n  note = {Just Accepted},\n  journal = {ACM Transactions on Software Engineering and Methodology}\n}27:T57c,Context: Pre-trained models (PTMs) have demonstrated significant potential in automatic code translation. However, the vulnerability of these models in translation tasks, particularly in terms of syntax, has not been extensively investigated. Objective: To fill this gap, our study aims to propose a novel approach CoTR to assess and improve the syntactic adversarial robustness of PTMs in code translation. Methods: CoTR consists of two components: CoTR-A and CoTR-D. CoTR-A generates adversarial examples by transforming programs, while CoTR-D proposes a semantic distance-based sampling data augmentation method and adversarial training method to improve the modelâ€™s robustness and generalization capabilities. The Pass@1 metric is used by CoTR to assess the performance of PTMs, which is more suitable for code translation tasks and offers a more precise evaluation in real-world scenarios. Results: The effectiveness of CoTR is evaluated through experiments on real-world Javaâ†”Python datasets. The results demonstrate that CoTR-A can significantly reduce the performance of existing PTMs, while CoTR-D effectively improves the robustness of PTMs. Conclusion: Our study identifies the limitations of current PTMs, including large language models, in code translation tasks. It highlights the potential of CoTR as an effective solution to enhance the robustness of PTMs for code translation tasks.28:T7ca,@article{YANG2025107699,\n  title = {Assessing and improving syntactic adversarial robustness of pre-trained models for code translation},\n  journal = {Information and Software Technology},\n  abbr = {IST'25},\n  tags = {SCI-Q2; CCF-B},\n  volume = {181},\n  pages = {107699},\n  year = {2025},\n  month = {5},\n  issn = {0950-5849},\n  doi = {https://doi.org/10.1016/j.infsof.2025.107699},\n  url = {https://www.sciencedirect.com/science/article/pii/S0950584925000382},\n  a"])</script><script>self.__next_f.push([1,"uthor = {Guang Yang and Yu Zhou and Xiangyu Zhang and Xiang Chen and Tingting Han and Taolue Chen},\n  abstract = {Context:\r\nPre-trained models (PTMs) have demonstrated significant potential in automatic code translation. However, the vulnerability of these models in translation tasks, particularly in terms of syntax, has not been extensively investigated.\r\nObjective:\r\nTo fill this gap, our study aims to propose a novel approach CoTR to assess and improve the syntactic adversarial robustness of PTMs in code translation.\r\nMethods:\r\nCoTR consists of two components: CoTR-A and CoTR-D. CoTR-A generates adversarial examples by transforming programs, while CoTR-D proposes a semantic distance-based sampling data augmentation method and adversarial training method to improve the modelâ€™s robustness and generalization capabilities. The Pass@1 metric is used by CoTR to assess the performance of PTMs, which is more suitable for code translation tasks and offers a more precise evaluation in real-world scenarios.\r\nResults:\r\nThe effectiveness of CoTR is evaluated through experiments on real-world Javaâ†”Python datasets. The results demonstrate that CoTR-A can significantly reduce the performance of existing PTMs, while CoTR-D effectively improves the robustness of PTMs.\r\nConclusion:\r\nOur study identifies the limitations of current PTMs, including large language models, in code translation tasks. It highlights the potential of CoTR as an effective solution to enhance the robustness of PTMs for code translation tasks.}\n}29:T856,"])</script><script>self.__next_f.push([1,"Code Language Models (CLMs), particularly those leveraging deep learning, have achieved significant success in code intelligence domain. However, the issue of security, particularly backdoor attacks, is often overlooked in this process. The previous research has focused on designing backdoor attacks for CLMs, but effective defenses have not been adequately addressed. In particular, existing defense methods from natural language processing, when directly applied to CLMs, are not effective enough and lack generality, working well in some models and scenarios but failing in others, thus fall short in consistently mitigating backdoor attacks. To bridge this gap, we first confirm the phenomenon of â€œearly learningâ€ as a general occurrence during the training of CLMs. This phenomenon refers to that a model initially focuses on the main features of training data but may become more sensitive to backdoor triggers over time, leading to overfitting and susceptibility to backdoor attacks. We then analyze that overfitting to backdoor triggers results from the use of the cross-entropy loss function, where the unboundedness of cross-entropy leads the model to increasingly concentrate on the features of the poisoned data. Based on this insight, we propose a general and effective loss function DeCE (Deceptive Cross-Entropy) by blending deceptive distributions and applying label smoothing to limit the gradient to bounded, which prevents the model from overfitting to backdoor triggers and then enhances the security of CLMs against backdoor attacks. To evaluate the effectiveness of our defense method, we select four code-related tasks as our experiments scenes and conduct experimental analyses on both natural language and two programming languages (Java and Python). Our experiments across multiple models with different sizes (from 125M to 7B) and poisoning ratios demonstrate the applicability and effectiveness of DeCE in enhancing the security of CLMs. The findings emphasize the potential of DeCE as a novel defense mechanism for CLMs, effectively tackling the challenge of securing models against backdoor threats."])</script><script>self.__next_f.push([1,"2a:Tac4,"])</script><script>self.__next_f.push([1,"@article{10.1145/3728639,\n  author = {Yang, Guang and Zhou, Yu and Zhang, Xiangyu and Chen, Xiang and Zhuo, Terry and Lo, David and Chen, Taolue},\n  abbr = {TOSEM'25},\n  tags = {SCI-Q1; CCF-A},\n  title = {Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss},\n  year = {2025},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  issn = {1049-331X},\n  url = {https://doi.org/10.1145/3728639},\n  doi = {10.1145/3728639},\n  abstract = {Code Language Models (CLMs), particularly those leveraging deep learning, have achieved significant success in code intelligence domain. However, the issue of security, particularly backdoor attacks, is often overlooked in this process. The previous research has focused on designing backdoor attacks for CLMs, but effective defenses have not been adequately addressed. In particular, existing defense methods from natural language processing, when directly applied to CLMs, are not effective enough and lack generality, working well in some models and scenarios but failing in others, thus fall short in consistently mitigating backdoor attacks. To bridge this gap, we first confirm the phenomenon of â€œearly learningâ€ as a general occurrence during the training of CLMs. This phenomenon refers to that a model initially focuses on the main features of training data but may become more sensitive to backdoor triggers over time, leading to overfitting and susceptibility to backdoor attacks. We then analyze that overfitting to backdoor triggers results from the use of the cross-entropy loss function, where the unboundedness of cross-entropy leads the model to increasingly concentrate on the features of the poisoned data. Based on this insight, we propose a general and effective loss function DeCE (Deceptive Cross-Entropy) by blending deceptive distributions and applying label smoothing to limit the gradient to bounded, which prevents the model from overfitting to backdoor triggers and then enhances the security of CLMs against backdoor attacks. To evaluate the effectiveness of our defense method, we select four code-related tasks as our experiments scenes and conduct experimental analyses on both natural language and two programming languages (Java and Python). Our experiments across multiple models with different sizes (from 125M to 7B) and poisoning ratios demonstrate the applicability and effectiveness of DeCE in enhancing the security of CLMs. The findings emphasize the potential of DeCE as a novel defense mechanism for CLMs, effectively tackling the challenge of securing models against backdoor threats.},\n  note = {Just Accepted},\n  journal = {ACM Transactions on Software Engineering and Methodology},\n  month = {5}\n}"])</script><script>self.__next_f.push([1,"2b:T633,The extensive application of Large Language Models (LLMs) in generative coding tasks has raised concerns due to their high computational demands and energy consumption. Unlike previous structural pruning methods designed for classification models that deal with lowdimensional classification logits, generative Code LLMs produce high-dimensional token logit sequences, making traditional pruning objectives inherently limited. Moreover, existing single component pruning approaches further constrain the effectiveness when applied to generative Code LLMs. In response, we propose Flab-Pruner, an innovative unified structural pruning method that combines vocabulary, layer, and Feed-Forward Network (FFN) pruning. This approach effectively reduces model parameters while maintaining performance. Additionally, we introduce a customized code instruction data strategy for coding tasks to enhance the performance recovery efficiency of the pruned model. Through extensive evaluations on three state-of-the-art Code LLMs across multiple generative coding tasks, the results demonstrate that Flab-Pruner retains 97% of the original performance after pruning 22% of the parameters and achieves the same or even better performance after post-training. The pruned models exhibit significant improvements in storage, GPU usage, computational efficiency, and environmental impact, while maintaining well robustness. Our research provides a sustainable solution for green software engineering and promotes the efficient deployment of LLMs in real-world generative coding intelligence applications.2c:T84c,"])</script><script>self.__next_f.push([1,"@article{yang2025moregreencodelarge,\n  title = {Less is More: Towards Green Code Large Language Models via Unified Structural Pruning},\n  abbr = {arXiv'25},\n  tags = {Preprint},\n  author = {Guang Yang and Yu Zhou and Xiangyu Zhang and Wei Cheng and Ke Liu and Xiang Chen and Terry Yue Zhuo and Taolue Chen},\n  year = {2025},\n  month = {4},\n  doi = {10.48550/arXiv.2412.15921},\n  eprint = {2412.15921},\n  archivePrefix = {arXiv},\n  journal = {arXiv},\n  primaryClass = {cs.SE},\n  url = {https://arxiv.org/abs/2412.15921},\n  abstract = {The extensive application of Large Language Models (LLMs) in generative coding tasks has raised concerns due to their high computational demands and energy consumption. Unlike previous structural pruning methods designed for classification models that deal with lowdimensional classification logits, generative Code LLMs produce high-dimensional token logit sequences, making traditional pruning objectives inherently limited. Moreover, existing single component pruning approaches further constrain the effectiveness when applied to generative Code LLMs. In response, we propose Flab-Pruner, an innovative unified structural pruning method that combines vocabulary, layer, and Feed-Forward Network (FFN) pruning. This approach effectively reduces model parameters while maintaining performance. Additionally, we introduce a customized code instruction data strategy for coding tasks to enhance the performance recovery efficiency of the pruned model. Through extensive evaluations on three state-of-the-art Code LLMs across multiple generative coding tasks, the results demonstrate that Flab-Pruner retains 97% of the original performance after pruning 22% of the parameters and achieves the same or even better performance after post-training. The pruned models exhibit significant improvements in storage, GPU usage, computational efficiency, and environmental impact, while maintaining well robustness. Our research provides a sustainable solution for green software engineering and promotes the efficient deployment of LLMs in real-world generative coding intelligence applications.}\n}"])</script><script>self.__next_f.push([1,"2d:T4f7,Regular expressions can describe specific matching rules, which can be used to determine the string format or extract the string content. Until now, regular expressions have been widely used in various operating systems and have been supported by most programming languages. However, the understanding of regular expressions is challenging for end-users or developers, who may not familiar with the syntax of regular expressions. Therefore, it is pressing for researchers to put forward effective methods to automatically generate functional descriptions for regular expressions. In this study, we formalize the description generation of the regular expression as a neural machine translation task. Then we propose a fully data-driven method RegexExplainer based on Transformer. Specifically, it uses Regex Encoder and AST Encoder to jointly learn the encoding information of regular expressions. In order to confirm the effectiveness of RegexExplainer, we build a high-quality corpus that consists of 107,851 pairs of regular expressions along with their corresponding descriptions. The final experimental results demonstrate that RegexExplainer is able to attain better performance compared to the state-of-the-art baselines when it comes to three performance measures.2e:T6fd,@INPROCEEDINGS{10928415,\n  author = {Wu, Yun and Cao, Shuangbo and Liu, Tianyue and Yang, Xiao and Yang, Guang},\n  booktitle = {Proceedings of the 4th International Conference on Communication Technology and Information Technology},\n  title = {RegexExplainer: Automatic Description Generation for Regular Expressions via Transformer},\n  year = {2024},\n  month = {12},\n  abbr = {ICCTIT'24},\n  tags = {EI},\n  volume = {},\n  number = {},\n  pages = {364-368},\n  doi = {10.1109/ICCTIT64404.2024.10928415},\n  abstract = {Regular expressions can describe specific matching rules, which can be used to determine the string format or extract the string content. Until now, regular expressions have been widely used in various operating systems and have been supported by"])</script><script>self.__next_f.push([1," most programming languages. However, the understanding of regular expressions is challenging for end-users or developers, who may not familiar with the syntax of regular expressions. Therefore, it is pressing for researchers to put forward effective methods to automatically generate functional descriptions for regular expressions. In this study, we formalize the description generation of the regular expression as a neural machine translation task. Then we propose a fully data-driven method RegexExplainer based on Transformer. Specifically, it uses Regex Encoder and AST Encoder to jointly learn the encoding information of regular expressions. In order to confirm the effectiveness of RegexExplainer, we build a high-quality corpus that consists of 107,851 pairs of regular expressions along with their corresponding descriptions. The final experimental results demonstrate that RegexExplainer is able to attain better performance compared to the state-of-the-art baselines when it comes to three performance measures.}\n}2f:T5cb,Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models ( â„“LMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most â„“LMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach COTTON which can leverage â„“LMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms o"])</script><script>self.__next_f.push([1,"f automated and human evaluation metrics. In particular, the CoTs generated by COTTON boost various â„“LMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that COTTON not only improves the performance of â„“LMs, but also enhances the performance of LLMs. Our study showcases the potential of â„“LMs in software engineering applications.30:T7a3,@ARTICLE{10634302,\n  author = {Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue},\n  abbr = {TSE'24},\n  tags = {SCI-Q1; CCF-A},\n  journal = {IEEE Transactions on Software Engineering},\n  title = {Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models},\n  year = {2024},\n  month = {12},\n  volume = {50},\n  number = {9},\n  pages = {2437-2457},\n  doi = {10.1109/TSE.2024.3440503},\n  abstract = {Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models ( â„“LMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most â„“LMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach COTTON which can leverage â„“LMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by COTTON bo"])</script><script>self.__next_f.push([1,"ost various â„“LMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that COTTON not only improves the performance of â„“LMs, but also enhances the performance of LLMs. Our study showcases the potential of â„“LMs in software engineering applications.}\n}31:T48e,Code generation aims to map natural language descriptions to code snippets. Recent approaches using sequence-to-tree models have shown promising results. However, they generally adopt an autoregressive way to predict the next token based on previous ones and do not consider potential future tokens. To address this issue, we propose Contextor, a novel context-sensitive model employing a bidirectional decoder to generate tokens in two different orders synchronously and interactively. Specifically, we employ two decoders to generate two sequences of different traversals and share their context knowledge via the attention mechanism. As a result, our model can synthesize both previous and future information simultaneously. To alleviate the information leakage problem caused by the teacher-forcing training strategy and bidirectional decoding, we propose an adapted scheduled sampling technique to prevent the decoders from contacting the actual label. Furthermore, Contextor also features a bidirectional beam search algorithm to better interact with both decoders. Experimental results demonstrate that our approach outperforms the state-of-the-art baselines.32:T69f,@article{ZHANG2024112066,\n  title = {Context-aware code generation with synchronous bidirectional decoder},\n  journal = {Journal of Systems and Software},\n  abbr = {JSS'24},\n  tags = {SCI-Q2; CCF-B},\n  volume = {214},\n  pages = {112066},\n  year = {2024},\n  month = {8},\n  issn = {0164-1212},\n  doi = {https://doi.org/10.1016/j.jss.2024.112066},\n  url = {https://www.sciencedirect.com/science/article/pii/S0164121224001110},\n  author = {Xiangyu Zhang and Yu Zhou and Guang Yang and"])</script><script>self.__next_f.push([1," Tingting Han and Taolue Chen},\n  abstract = {Code generation aims to map natural language descriptions to code snippets. Recent approaches using sequence-to-tree models have shown promising results. However, they generally adopt an autoregressive way to predict the next token based on previous ones and do not consider potential future tokens. To address this issue, we propose Contextor, a novel context-sensitive model employing a bidirectional decoder to generate tokens in two different orders synchronously and interactively. Specifically, we employ two decoders to generate two sequences of different traversals and share their context knowledge via the attention mechanism. As a result, our model can synthesize both previous and future information simultaneously. To alleviate the information leakage problem caused by the teacher-forcing training strategy and bidirectional decoding, we propose an adapted scheduled sampling technique to prevent the decoders from contacting the actual label. Furthermore, Contextor also features a bidirectional beam search algorithm to better interact with both decoders. Experimental results demonstrate that our approach outperforms the state-of-the-art baselines.}\n}33:T7a9,When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later w"])</script><script>self.__next_f.push([1,"e fine-tune the pre-trained language model CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts). To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. In addition, our ablation studies also confirm the effectiveness of component settings (such as bi-modal information, prompt learning, hybrid prompts, and multi-task learning) of SOTitle+. Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction.34:T978,"])</script><script>self.__next_f.push([1,"@article{yang2024automatic,\n  title = {Automatic bi-modal question title generation for Stack Overflow with prompt learning},\n  abbr = {EMSE'24},\n  tags = {SCI-Q2; CCF-B},\n  author = {Yang, Shaoyu and Chen, Xiang and Liu, Ke and Yang, Guang and Yu, Chi},\n  journal = {Empirical Software Engineering},\n  volume = {29},\n  number = {3},\n  pages = {63},\n  year = {2024},\n  month = {5},\n  publisher = {Springer},\n  doi = {10.1007/s10664-024-10466-4},\n  abstract = {When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts). To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. In addition, our ablation studies also confirm the effectiveness of component settings (such as bi-modal information, prompt learning, hybrid prompts, and multi-task learning) of SOTitle+. Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction.}\n}"])</script><script>self.__next_f.push([1,"35:T852,"])</script><script>self.__next_f.push([1,"Context: Designing effective automatic smart contract comment generation approaches can facilitate developersâ€™ comprehension, boosting smart contract development and improving vulnerability detection. The previous approaches can be divided into two categories: fine-tuning paradigm-based approaches and information retrieval-based approaches. Objective: However, for the fine-tuning paradigm-based approaches, the performance may be limited by the quality of the gathered dataset for the downstream task and they may have knowledge-forgetting issues, which can reduce the generality of the fine-tuned model. While for the information retrieval-based approaches, it is difficult for them to generate high-quality comments if similar code does not exist in the historical repository. Therefore we want to utilize the domain knowledge related to smart contract code comment generation in large language models (LLMs) to alleviate the disadvantages of these two types of approaches. Method: In this study, we propose an approach SCCLLM based on LLMs and in-context learning. Specifically, in the demonstration selection phase, SCCLLM retrieves the top- code snippets from the historical corpus by considering syntax, semantics, and lexical information. In the in-context learning phase, SCCLLM utilizes the retrieved code snippets as demonstrations for in-context learning, which can help to utilize the related knowledge for this task in the LLMs. In the LLMs inference phase, the input is the target smart contract code snippet, and the output is the corresponding comment generated by the LLMs. Results: We select a large corpus from a smart contract community Etherscan.io as our experimental subject. Extensive experimental results show the effectiveness of SCCLLM when compared with baselines in automatic evaluation and human evaluation. We also show the rationality of our customized demonstration selection strategy in SCCLLM by ablation studies. Conclusion: Our study shows using LLMs and in-context learning is a promising direction for automatic smart contract comment generation, which calls for more follow-up studies."])</script><script>self.__next_f.push([1,"36:Ta29,"])</script><script>self.__next_f.push([1,"@article{zhao2024automatic,\n  title = {Automatic smart contract comment generation via large language models and in-context learning},\n  abbr = {IST'24},\n  tags = {SCI-Q2; CCF-B},\n  author = {Zhao, Junjie and Chen, Xiang and Yang, Guang and Shen, Yiheng},\n  journal = {Information and Software Technology},\n  volume = {168},\n  pages = {107405},\n  year = {2024},\n  month = {4},\n  publisher = {Elsevier},\n  doi = {10.1016/j.infsof.2024.107405},\n  abstract = {Context:\r\nDesigning effective automatic smart contract comment generation approaches can facilitate developersâ€™ comprehension, boosting smart contract development and improving vulnerability detection. The previous approaches can be divided into two categories: fine-tuning paradigm-based approaches and information retrieval-based approaches.\r\nObjective:\r\nHowever, for the fine-tuning paradigm-based approaches, the performance may be limited by the quality of the gathered dataset for the downstream task and they may have knowledge-forgetting issues, which can reduce the generality of the fine-tuned model. While for the information retrieval-based approaches, it is difficult for them to generate high-quality comments if similar code does not exist in the historical repository. Therefore we want to utilize the domain knowledge related to smart contract code comment generation in large language models (LLMs) to alleviate the disadvantages of these two types of approaches.\r\nMethod:\r\nIn this study, we propose an approach SCCLLM based on LLMs and in-context learning. Specifically, in the demonstration selection phase, SCCLLM retrieves the top-\r\n code snippets from the historical corpus by considering syntax, semantics, and lexical information. In the in-context learning phase, SCCLLM utilizes the retrieved code snippets as demonstrations for in-context learning, which can help to utilize the related knowledge for this task in the LLMs. In the LLMs inference phase, the input is the target smart contract code snippet, and the output is the corresponding comment generated by the LLMs.\r\nResults:\r\nWe select a large corpus from a smart contract community Etherscan.io as our experimental subject. Extensive experimental results show the effectiveness of SCCLLM when compared with baselines in automatic evaluation and human evaluation. We also show the rationality of our customized demonstration selection strategy in SCCLLM by ablation studies.\r\nConclusion:\r\nOur study shows using LLMs and in-context learning is a promising direction for automatic smart contract comment generation, which calls for more follow-up studies.}\n}"])</script><script>self.__next_f.push([1,"37:T668,Understanding Bash code is challenging for developers due to its syntax flexibility and unique features. Bash lacks sufficient training data compared to comment generation tasks in popular programming languages. Furthermore, collecting more real Bash code and corresponding comments is time-consuming and labor-intensive. In this study, we propose a two-module method named Bash2Com for Bash code comments generation. The first module, NP-GD, is a gradient-based automatic data augmentation component that enhances normalization stability when generating adversarial examples. The second module, MASA, leverages CodeBERT to learn the rich semantics of Bash code. Specifically, MASA considers the representations learned at each layer of CodeBERT as a set of semantic information that captures recursive relationships within the code. To generate comments for different Bash snippets, MASA employs LSTM and attention mechanisms to dynamically concentrate on relevant representational information. Then, we utilize the Transformer decoder and beam search algorithm to generate code comments. To evaluate the effectiveness of Bash2Com, we consider a corpus of 10,592 Bash code and corresponding comments. Compared with the state-of-the-art baselines, our experimental results show that Bash2Com can outperform all baselines by at least 10.19%, 11.81%, 2.61%, and 6.13% in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L. Moreover, the rationality of NP-GD and MASA in Bash2Com are verified by ablation studies. Finally, we conduct a human evaluation to illustrate the effectiveness of Bash2Com from practitioners' perspectives.38:T81f,"])</script><script>self.__next_f.push([1,"@article{shen2024bash,\n  title = {Bash comment generation via data augmentation and semantic-aware CodeBERT},\n  author = {Shen, Yiheng and Ju, Xiaolin and Chen, Xiang and Yang, Guang},\n  abbr = {ASEJ'24},\n  tags = {SCI-Q2; CCF-B},\n  journal = {Automated Software Engineering},\n  volume = {31},\n  number = {1},\n  pages = {30},\n  year = {2024},\n  month = {3},\n  publisher = {Springer},\n  abstract = {Understanding Bash code is challenging for developers due to its syntax flexibility and unique features. Bash lacks sufficient training data compared to comment generation tasks in popular programming languages. Furthermore, collecting more real Bash code and corresponding comments is time-consuming and labor-intensive. In this study, we propose a two-module method named Bash2Com for Bash code comments generation. The first module, NP-GD, is a gradient-based automatic data augmentation component that enhances normalization stability when generating adversarial examples. The second module, MASA, leverages CodeBERT to learn the rich semantics of Bash code. Specifically, MASA considers the representations learned at each layer of CodeBERT as a set of semantic information that captures recursive relationships within the code. To generate comments for different Bash snippets, MASA employs LSTM and attention mechanisms to dynamically concentrate on relevant representational information. Then, we utilize the Transformer decoder and beam search algorithm to generate code comments. To evaluate the effectiveness of Bash2Com, we consider a corpus of 10,592 Bash code and corresponding comments. Compared with the state-of-the-art baselines, our experimental results show that Bash2Com can outperform all baselines by at least 10.19%, 11.81%, 2.61%, and 6.13% in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L. Moreover, the rationality of NP-GD and MASA in Bash2Com are verified by ablation studies. Finally, we conduct a human evaluation to illustrate the effectiveness of Bash2Com from practitioners' perspectives.},\n  doi = {10.1007/s10515-024-00431-2}\n}"])</script><script>self.__next_f.push([1,"39:T695,Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neuRAl coDe generAtor Robustifier (RADAR). RADAR consists of two components: RADAR-Attack and RADAR-Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR-Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR-Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.3a:T8a0,"])</script><script>self.__next_f.push([1,"@article{yang2024important,\n  title = {How important are good method names in neural code generation? a model robustness perspective},\n  abbr = {TOSEM'24},\n  tags = {SCI-Q1; CCF-A},\n  month = {3},\n  author = {Yang, Guang and Zhou, Yu and Yang, Wenhua and Yue, Tao and Chen, Xiang and Chen, Taolue},\n  journal = {ACM Transactions on Software Engineering and Methodology},\n  volume = {33},\n  number = {3},\n  pages = {1--35},\n  year = {2024},\n  publisher = {ACM New York, NY, USA},\n  doi = {10.1145/3630010},\n  abstract = {Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neuRAl coDe generAtor Robustifier (RADAR). RADAR consists of two components: RADAR-Attack and RADAR-Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR-Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR-Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.}\n}"])</script><script>self.__next_f.push([1,"3b:T550,Evaluation metrics are crucial in the field of code synthesis. Commonly used code evaluation metrics can be classified into three types: match-based, semantic-based, and execution-based. Among them, the execution-based Pass@k metric accurately assesses the functionality of predicted code by executing test cases. However, calculating this metric requires a significant amount of overhead, necessitating the design of an automated evaluation metric that can assess the functionality of predicted code without the need for test cases. Additionally, a good evaluation metric should be robust, that is the metric can maintain its accuracy even when the predicted code undergoes minor changes. To address these challenges, we propose an automated robust metric, called CodeScore-R, based on UniXcoder and contrastive learning, for evaluating the functionality of code synthesis. CodeScore-R employs techniques such as sketch-based processing, syntactic-equivalent transformations, and mutation testing to effectively mitigate the interference caused by identifiers, syntax structures, and operators on evaluation results. Experimental results demonstrate that in the tasks of code generation and migration in Java and Python, CodeScore-R outperforms other evaluation metrics and is more closely aligned with the Pass@k metric, while exhibiting stronger robustness.3c:T773,@Article{2023-30715,\n  title = {CodeScore-Rï¼šç”¨äºŽè¯„ä¼°ä»£ç åˆæˆåŠŸèƒ½å‡†ç¡®æ€§çš„è‡ªåŠ¨åŒ–é²æ£’æŒ‡æ ‡},\n  journal = {è®¡ç®—æœºç ”ç©¶ä¸Žå‘å±•},\n  abbr = {è®¡ç®—æœºç ”ç©¶ä¸Žå‘å±•'24},\n  tags = {EI; ä¸­æ–‡CCF-A; åŒ—æ ¸},\n  volume = {61},\n  number = {2},\n  pages = {291-306},\n  year = {2024},\n  month = {2},\n  issn = {1000-1239},\n  doi = {10.7544/issn1000-1239.202330715},\n  url = {https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330715},\n  author = {Yang Guang and Zhou Yu and Chen Xiang and Zhang Xiangyu},\n  abstract = {Evaluation metrics are crucial in the field of code synthesis. Commonly used code evaluation metrics can be classified into thre"])</script><script>self.__next_f.push([1,"e types: match-based, semantic-based, and execution-based. Among them, the execution-based Pass@k metric accurately assesses the functionality of predicted code by executing test cases. However, calculating this metric requires a significant amount of overhead, necessitating the design of an automated evaluation metric that can assess the functionality of predicted code without the need for test cases. Additionally, a good evaluation metric should be robust, that is the metric can maintain its accuracy even when the predicted code undergoes minor changes. To address these challenges, we propose an automated robust metric, called CodeScore-R, based on UniXcoder and contrastive learning, for evaluating the functionality of code synthesis. CodeScore-R employs techniques such as sketch-based processing, syntactic-equivalent transformations, and mutation testing to effectively mitigate the interference caused by identifiers, syntax structures, and operators on evaluation results. Experimental results demonstrate that in the tasks of code generation and migration in Java and Python, CodeScore-R outperforms other evaluation metrics and is more closely aligned with the Pass@k metric, while exhibiting stronger robustness.}\n}3d:T41b,Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with token-level retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them directly to code generation experience challenges: the use of the retrieval-based mechanism inevitably introduces extraneous noise to the generation process, resulting in even syntactically incorrect code. Computationally, such models necessitate frequent searches of the cached datastore, which turns out to be time-consuming. To address these issues, we propose $k$NN-TRANX, a token-level retrieval augmented code generation method. $k$NN-TRANX allows for searches in smaller datastores tailored for the cod"])</script><script>self.__next_f.push([1,"e generation task. It leverages syntax constraints for the retrieval of datastores, which reduces the impact of retrieve noise. We evaluate $k$NN-TRANX on two public datasets and the experimental results confirm the effectiveness of our approach.3e:T6a0,@inproceedings{zhang-etal-2023-syntax,\n  title = {Syntax-Aware Retrieval Augmented Code Generation},\n  abbr = {EMNLP'23},\n  tags = {CCF-B;EI},\n  author = {Zhang, Xiangyu  and\r\n  Zhou, Yu  and\r\n  Yang, Guang  and\r\n  Chen, Taolue},\n  editor = {Bouamor, Houda  and\r\n  Pino, Juan  and\r\n  Bali, Kalika},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP},\n  month = {12},\n  year = {2023},\n  address = {Singapore},\n  publisher = {Association for Computational Linguistics},\n  url = {https://aclanthology.org/2023.findings-emnlp.90/},\n  doi = {10.18653/v1/2023.findings-emnlp.90},\n  pages = {1291--1302},\n  abstract = {Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with token-level retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them directly to code generation experience challenges: the use of the retrieval-based mechanism inevitably introduces extraneous noise to the generation process, resulting in even syntactically incorrect code. Computationally, such models necessitate frequent searches of the cached datastore, which turns out to be time-consuming. To address these issues, we propose $k$NN-TRANX, a token-level retrieval augmented code generation method. $k$NN-TRANX allows for searches in smaller datastores tailored for the code generation task. It leverages syntax constraints for the retrieval of datastores, which reduces the impact of retrieve noise. We evaluate $k$NN-TRANX on two public datasets and the experimental results confirm the effectiveness of our approach.}\n}3f:T5fd,Due to the development of pre-trained language models, automated code generation technique"])</script><script>self.__next_f.push([1,"s have shown great promise in recent years. However, the generated code will not always adhere to syntactic constraints of the target language, especially in the case of Turducken-style code, where declarative code snippets are embedded within imperative programs. In this study, we summarize three significant challenges in regards to syntactic constraints: (1) the efficient representation of syntactic constraints, (2) the effective integration of syntactic information, and (3) the scalable syntax-first decoding algorithm. To address these challenges, we propose a syntax-guided multi-task learning approach TurduckenGen. Specifically, we first explicitly append the type information to the code tokens to capture the representation of syntactic constraints. Then we formalize code generation with syntactic constraint representation as an auxiliary task to enable the model to learn the syntactic constraints of the code. Finally, the syntactically correct code is selected accurately from the multiple candidates with the help of the compiler feedback. Extensive experiments and comprehensive analysis demonstrate the effectiveness and general applicability of our approach after being compared with six state-of-the-art baselines on two Turducken-style code datasets. Finally, we conducted a human study and found the code quality generated by our approach is better than baselines in terms of code readability and semantic similarity.40:T7ef,@article{yang2023syntax,\n  title = {A syntax-guided multi-task learning approach for Turducken-style code generation},\n  abbr = {EMSE'23},\n  tags = {SCI-Q2; CCF-B},\n  author = {Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Xu, Yiran and Han, Tingting and Chen, Taolue},\n  journal = {Empirical Software Engineering},\n  volume = {28},\n  number = {6},\n  pages = {141},\n  year = {2023},\n  publisher = {Springer},\n  month = {10},\n  doi = {10.1007/s10664-023-10372-1},\n  abstract = {Due to the development of pre-trained language models, automated code generation techniques have shown"])</script><script>self.__next_f.push([1," great promise in recent years. However, the generated code will not always adhere to syntactic constraints of the target language, especially in the case of Turducken-style code, where declarative code snippets are embedded within imperative programs. In this study, we summarize three significant challenges in regards to syntactic constraints: (1) the efficient representation of syntactic constraints, (2) the effective integration of syntactic information, and (3) the scalable syntax-first decoding algorithm. To address these challenges, we propose a syntax-guided multi-task learning approach TurduckenGen. Specifically, we first explicitly append the type information to the code tokens to capture the representation of syntactic constraints. Then we formalize code generation with syntactic constraint representation as an auxiliary task to enable the model to learn the syntactic constraints of the code. Finally, the syntactically correct code is selected accurately from the multiple candidates with the help of the compiler feedback. Extensive experiments and comprehensive analysis demonstrate the effectiveness and general applicability of our approach after being compared with six state-of-the-art baselines on two Turducken-style code datasets. Finally, we conducted a human study and found the code quality generated by our approach is better than baselines in terms of code readability and semantic similarity.}\n}41:T489,Software defect prediction (SDP) is a critical task that aims to identify potential defects and allocate resources for testing to enhance software reliability. In this study, we present a novel defect prediction framework called EDP-BGCNN, which leverages the power of BERT and graph convolutional neural networks to represent code. Our approach first extracts the codeâ€™s structural semantic features based on its abstract syntax tree (AST), followed by applying BERT for embedded learning to extract the codeâ€™s semantic features. We then use latent Dirichlet allocation (LDA) to extract descriptive se"])</script><script>self.__next_f.push([1,"mantic features and convert them into a numeric vector. The code and descriptive semantic features are then combined and processed by GraphSMOTE to address the class imbalance problem. Finally, we obtain a more comprehensive representation using graph convolutional neural networks. We evaluated our approach on five open-source projects and compared it with three state-of-the-art deep-learning methods. Our experimental results demonstrate that EDP-BGCNN can achieve significant improvements in AUC (4.9% - 23%) and F1-measure (6.6% - 10.7%) on average.42:T64f,@INPROCEEDINGS{10196969,\n  author = {Shen, Hao and Ju, Xiaolin and Chen, Xiang and Yang, Guang},\n  booktitle = {Proceedings of the 47th Annual Computers, Software, and Applications Conference},\n  abbr = {COMPSAC'23},\n  tags = {EI; CCF-C},\n  title = {EDP-BGCNN: Effective Defect Prediction via BERT-based Graph Convolutional Neural Network},\n  year = {2023},\n  month = {8},\n  pages = {850-859},\n  doi = {10.1109/COMPSAC57700.2023.00114},\n  abstract = {Software defect prediction (SDP) is a critical task that aims to identify potential defects and allocate resources for testing to enhance software reliability. In this study, we present a novel defect prediction framework called EDP-BGCNN, which leverages the power of BERT and graph convolutional neural networks to represent code. Our approach first extracts the codeâ€™s structural semantic features based on its abstract syntax tree (AST), followed by applying BERT for embedded learning to extract the codeâ€™s semantic features. We then use latent Dirichlet allocation (LDA) to extract descriptive semantic features and convert them into a numeric vector. The code and descriptive semantic features are then combined and processed by GraphSMOTE to address the class imbalance problem. Finally, we obtain a more comprehensive representation using graph convolutional neural networks. We evaluated our approach on five open-source projects and compared it with three state-of-the-art deep-learning methods. Our experimental result"])</script><script>self.__next_f.push([1,"s demonstrate that EDP-BGCNN can achieve significant improvements in AUC (4.9% - 23%) and F1-measure (6.6% - 10.7%) on average.}\n}43:T6ce,@inproceedings{DBLP:conf/seke/ShenJ0Y23,\n  author = {Yiheng Shen and\r\n                  Xiaolin Ju and\r\n                  Xiang Chen and\r\n                  Guang Yang},\n  abbr = {SEKE'23},\n  tags = {EI; CCF-C},\n  editor = {Shi{-}Kuo Chang},\n  title = {An Empirical Study of Adversarial Training in Code Comment Generation},\n  booktitle = {Proceedings of the 35th International Conference on Software Engineering and Knowledge Engineering},\n  pages = {292--297},\n  publisher = {{KSI} Research Inc.},\n  year = {2023},\n  month = {7},\n  url = {https://doi.org/10.18293/SEKE2023-108},\n  doi = {10.18293/SEKE2023-108},\n  timestamp = {Tue, 15 Oct 2024 08:05:55 +0200},\n  biburl = {https://dblp.org/rec/conf/seke/ShenJ0Y23.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  abstract = {The code comment generation task is designed for developers to understand programs more quickly during development and maintenance. However, the existing automatic code comment generation models can not generate valuable comments for developers. It is necessary to explore a technology that can optimize the performance of code comment generation models without changing the model. We consider adversarial training as the experimental object, which can improve the robustness and generalization of the model. We present a large-scale study to experimentally validate the performance of gradient-based adversarial training methods in the code comment generation task. The results show that adversarial training can improve the model performance by generating adversarial examples without changing the model. Our empirical study can provide a new perspective for researchers to improve the performance of code comment generation models.}\n}44:T6ad,@inproceedings{Zhang2023CCGRA,\n  author = {Zhenhua Zhang, Shizhan Chen, Guodong Fan, Guang Yang, Zhiyong Feng},\n  abbr = {SEKE'23},\n  tags = {EI; CCF-C},\n  edit"])</script><script>self.__next_f.push([1,"or = {Shi{-}Kuo Chang},\n  title = {CCGRA: Smart Contract Code Comment Generation with Retrieval-enhanced Approach},\n  booktitle = {Proceedings of the 35th International Conference on Software Engineering and Knowledge Engineering},\n  pages = {212--217},\n  publisher = {{KSI} Research Inc.},\n  year = {2023},\n  url = {https://doi.org/10.18293/SEKE2023-090},\n  doi = {10.18293/SEKE2023-090},\n  timestamp = {Wed, 06 Sep 2023 16:44:32 +0200},\n  biburl = {https://dblp.org/rec/conf/seke/ZhangCFY023.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  month = {7},\n  abstract = {Smart contracts are self-executing programs on the blockchain that are critical to a range of industries, including finance, supply chain management, and healthcare. However, comprehending smart contracts can be challenging due to a lack of effective comments in most user-defined code. To address this challenge, we propose a novel retrieval-enhanced approach CCGRA that leverages retrieval knowledge to generate high-quality comments for Solidity language code. Our approach carefully eliminates duplicated data and template data in the widely-used smart contract dataset to ensure a high-quality corpus. Extensive experiments and comprehensive analysis demonstrate the effectiveness applicability of our approach after being compared with eight state-of-the-art baselines. Finally, we conduct a human study and find the comment quality generated by our approach is better than baselines in terms of similarity, naturalness, and informativeness.}\n}45:T624,Bash is the default shell command language for Linux, which plays an important role in the development and maintenance of Linux systems. Nevertheless, understanding the purpose and functionality of the Bash code is a challenging task. Therefore, an automatic method ExplainBash is proposed based on dual information retrieval for automatic Bash code comment generation. Specifically, the proposed method is based on semantic similarity and lexical similarity to perform dual information retr"])</script><script>self.__next_f.push([1,"ieval, which aims to generate high-quality code comments. For semantic similarity, CodeBERT and BERT-whitening operator are used to learn the code semantic representation, and Euclidean distance is resorted to compute semantic similarity; while for lexical similarity, code is represented as a set of code tokens, then the edit distance is resorted to compute lexical similarity. A high-quality corpus is constructed based on the corpus shared in the NL2Bash study and the data shared in the NLC2CMD competition. After that, nine state-of-the-art baselines are selected from the automatic code comment generation domain, which cover the information retrieval-based methods and deep learning-based methods. Results of empirical study and human study verify the effectiveness of the proposed method. Ablation experiments are also designed to analyze the rationality of the settings (such as retrieval strategy, BERT-whitening operator) in the proposed method. Finally, a browser plug-in is developed based on the proposed method to facilitate the code comprehension of the Bash code.46:T7ed,@Article{20231310,\n  title = {åŸºäºŽåŒé‡ä¿¡æ¯æ£€ç´¢çš„Bashä»£ç æ³¨é‡Šç”Ÿæˆæ–¹æ³•},\n  author = {Xiang, Chen and Yu, Chi and Yang, Guang and Pu, Xuelian and Cui, Zhanqi},\n  journal = {è½¯ä»¶å­¦æŠ¥},\n  abbr = {è½¯ä»¶å­¦æŠ¥'23},\n  tags = {EI; ä¸­æ–‡CCF-A; åŒ—æ ¸},\n  volume = {34},\n  number = {3},\n  pages = {1310},\n  numpages = {20},\n  year = {2023},\n  month = {03},\n  doi = {10.13328/j.cnki.jos.006690},\n  publisher = {ç§‘å­¦å‡ºç‰ˆç¤¾},\n  abstract = {Bash is the default shell command language for Linux, which plays an important role in the development and maintenance of Linux systems. Nevertheless, understanding the purpose and functionality of the Bash code is a challenging task. Therefore, an automatic method ExplainBash is proposed based on dual information retrieval for automatic Bash code comment generation. Specifically, the proposed method is based on semantic similarity and lexical similarity to perform dual information retrieval, which a"])</script><script>self.__next_f.push([1,"ims to generate high-quality code comments. For semantic similarity, CodeBERT and BERT-whitening operator are used to learn the code semantic representation, and Euclidean distance is resorted to compute semantic similarity; while for lexical similarity, code is represented as a set of code tokens, then the edit distance is resorted to compute lexical similarity. A high-quality corpus is constructed based on the corpus shared in the NL2Bash study and the data shared in the NLC2CMD competition. After that, nine state-of-the-art baselines are selected from the automatic code comment generation domain, which cover the information retrieval-based methods and deep learning-based methods. Results of empirical study and human study verify the effectiveness of the proposed method. Ablation experiments are also designed to analyze the rationality of the settings (such as retrieval strategy, BERT-whitening operator) in the proposed method. Finally, a browser plug-in is developed based on the proposed method to facilitate the code comprehension of the Bash code.}\n}47:T51d,Exploit code is widely used for detecting vulnerabilities and implementing defensive measures. However, automatic generation of exploit code for security assessment is a challenging task. In this paper, we propose a novel template-augmented exploit code generation approach ExploitGen based on CodeBERT. Specifically, we first propose a rule-based Template Parser to generate template-augmented natural language descriptions (NL). Both the raw and template-augmented NL sequences are encoded to context vectors by the respective encoders. For better learning semantic information, ExploitGen incorporates a semantic attention layer, which uses the attention mechanism to extract and calculate each layerâ€™s representational information. In addition, ExploitGen computes the interaction information between the template information and the semantics of the raw NL and designs a residual connection to append the template information into the semantics of the raw NL. Com"])</script><script>self.__next_f.push([1,"prehensive experiments on two datasets show the effectiveness of ExploitGen after comparison with six state-of-the-art baselines. Apart from the automatic evaluation, we conduct a human study to evaluate the quality of generated code in terms of syntactic and semantic correctness. The results also confirm the effectiveness of ExploitGen.48:T740,@article{YANG2023111577,\n  title = {ExploitGen: Template-augmented exploit code generation based on CodeBERT},\n  abbr = {JSS'23},\n  tags = {SCI-Q2; CCF-B},\n  journal = {Journal of Systems and Software},\n  volume = {197},\n  pages = {111577},\n  year = {2023},\n  month = {3},\n  issn = {0164-1212},\n  doi = {https://doi.org/10.1016/j.jss.2022.111577},\n  url = {https://www.sciencedirect.com/science/article/pii/S0164121222002539},\n  author = {Guang Yang and Yu Zhou and Xiang Chen and Xiangyu Zhang and Tingting Han and Taolue Chen},\n  abstract = {Exploit code is widely used for detecting vulnerabilities and implementing defensive measures. However, automatic generation of exploit code for security assessment is a challenging task. In this paper, we propose a novel template-augmented exploit code generation approach ExploitGen based on CodeBERT. Specifically, we first propose a rule-based Template Parser to generate template-augmented natural language descriptions (NL). Both the raw and template-augmented NL sequences are encoded to context vectors by the respective encoders. For better learning semantic information, ExploitGen incorporates a semantic attention layer, which uses the attention mechanism to extract and calculate each layerâ€™s representational information. In addition, ExploitGen computes the interaction information between the template information and the semantics of the raw NL and designs a residual connection to append the template information into the semantics of the raw NL. Comprehensive experiments on two datasets show the effectiveness of ExploitGen after comparison with six state-of-the-art baselines. Apart from the automatic evaluation, we conduct a human s"])</script><script>self.__next_f.push([1,"tudy to evaluate the quality of generated code in terms of syntactic and semantic correctness. The results also confirm the effectiveness of ExploitGen.}\n}49:T795,Developers use shell commands for many tasks, such as file system management, network control, and process management. Bash is one of the most commonly used shells and plays an important role in Linux system development and maintenance. Due to the language flexibility of Bash code, developers who are not familiar with Bash often have difficulty understanding the purpose and functionality of Bash code. In this study, we study Bash code comment generation problem and proposed an automatic method BASHEXPLAINER based on two-stage training strategy. In the first stage, we train a Bash encoder by fine-tuning CodeBERT on our constructed Bash code corpus. In the second stage, we first retrieve the most similar code from the code repository for the target code based on semantic and lexical similarity. Then we use the trained Bash encoder to generate two vector representations. Finally, we fuse these two vector representations via the fusion layer and generate the code comment through the decoder. To show the competitiveness of our proposed method, we construct a high-quality corpus by combining the corpus shared in the previous NL2Bash study and the corpus shared in the NLC2CMD competition. This corpus contains 10,592 Bash codes and corresponding comments. Then we selected ten baselines from previous studies on automatic code comment generation, which cover information retrieval methods, deep learning methods, and hybrid methods. The experimental results show that in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L, BASHEXPLAINER can outperform all baselines by at least 8.75%, 9.29%, 4.77% and 3.86%. Then we design ablation experiments to show the component setting rationality of BASHEXPLAINER. Later, we conduct a human study to further show the competitiveness of BASHEXPLAINER. Finally, we develop a browser plug-in based on BASHEXPLAINER to facil"])</script><script>self.__next_f.push([1,"itate the understanding of the Bash code for developers.4a:T9a4,"])</script><script>self.__next_f.push([1,"@INPROCEEDINGS{9978203,\n  author = {Yu, Chi and Yang, Guang and Chen, Xiang and Liu, Ke and Zhou, Yanlin},\n  booktitle = {Proceedings of the 2022 IEEE International Conference on Software Maintenance and Evolution},\n  abbr = {ICSME'22},\n  tags = {EI; CCF-B},\n  title = {BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT},\n  year = {2022},\n  volume = {},\n  number = {},\n  pages = {82-93},\n  abstract = {Developers use shell commands for many tasks, such as file system management, network control, and process management. Bash is one of the most commonly used shells and plays an important role in Linux system development and maintenance. Due to the language flexibility of Bash code, developers who are not familiar with Bash often have difficulty understanding the purpose and functionality of Bash code. In this study, we study Bash code comment generation problem and proposed an automatic method BASHEXPLAINER based on two-stage training strategy. In the first stage, we train a Bash encoder by fine-tuning CodeBERT on our constructed Bash code corpus. In the second stage, we first retrieve the most similar code from the code repository for the target code based on semantic and lexical similarity. Then we use the trained Bash encoder to generate two vector representations. Finally, we fuse these two vector representations via the fusion layer and generate the code comment through the decoder. To show the competitiveness of our proposed method, we construct a high-quality corpus by combining the corpus shared in the previous NL2Bash study and the corpus shared in the NLC2CMD competition. This corpus contains 10,592 Bash codes and corresponding comments. Then we selected ten baselines from previous studies on automatic code comment generation, which cover information retrieval methods, deep learning methods, and hybrid methods. The experimental results show that in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L, BASHEXPLAINER can outperform all baselines by at least 8.75%, 9.29%, 4.77% and 3.86%. Then we design ablation experiments to show the component setting rationality of BASHEXPLAINER. Later, we conduct a human study to further show the competitiveness of BASHEXPLAINER. Finally, we develop a browser plug-in based on BASHEXPLAINER to facilitate the understanding of the Bash code for developers.},\n  doi = {10.1109/ICSME55016.2022.00016},\n  ISSN = {2576-3148},\n  month = {10}\n}"])</script><script>self.__next_f.push([1,"4b:T5d5,@inproceedings{DBLP:conf/icbase/TianWY22,\n  author = {Xinyi Tian and\r\n                  Jingkun Wu and\r\n                  Guang Yang},\n  abbr = {ICBASE'22},\n  tags = {EI},\n  editor = {Shaozhang Niu and\r\n                  Jun Sang},\n  title = {{BUG-T5:} {A} Transformer-based Automatic Title Generation Method\r\n                  for Bug Reports},\n  booktitle = {Proceedings of the 3rd International Conference on Big Data {\\\u0026}\r\n                  Artificial Intelligence {\\\u0026} Software Engineering},\n  series = {{CEUR} Workshop Proceedings},\n  volume = {3304},\n  pages = {45--50},\n  publisher = {CEUR-WS.org},\n  year = {2022},\n  url = {https://ceur-ws.org/Vol-3304/paper06.pdf},\n  timestamp = {Fri, 11 Oct 2024 13:17:02 +0200},\n  biburl = {https://dblp.org/rec/conf/icbase/TianWY22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  month = {10},\n  abstract = {In Github, developers may not clarify and summarize the critical problems in the bug report titles due to a lack of domain knowledge or poor writing skills. Therefore, it is essential to help practitioners draft high-quality titles. In this study, we propose the BUG-T5 method automatically generating titles by fine-tuning the T5 model. In our empirical analysis, we choose a publicly available corpus from Github. After comparing BUG-T5 with four state-ofthe-art baselines (i.e., TextRank, NMT, Transformer, and iTAPE) on ROUGE metrics, we demonstrate the competitiveness of our proposed method, BUG-T5.}\n}4c:T67d,With the development of deep learning and natural language processing techniques, the performance of many source code-related tasks can be improved by using pre-trained models. Of these pre-trained models, CodeBert is a bi-modal pre-trained model for programming languages and natural languages, which has been successfully used in current source code-related tasks. These previous studies mainly use the output vector of CodeBertâ€™s last layer as the code semantic representation for fine-tuning downstream source code-related tasks. How"])</script><script>self.__next_f.push([1,"ever, this setting may miss the valuable representational information, which may be captured by other layers of CodeBert. To better exploit the representational information in each layer of CodeBert for fine-tuning downstream source code-related tasks, we propose an approach EL-CodeBert. Our approach first extracts the representational information in each layer of CodeBert and views them as a representational information sequence. Then our approach learns the importance of representational information in each layer through the bidirectional recurrent neural network (i.e., Bi-LSTM) and the attention mechanism. To verify the effectiveness of our proposed approach, we select four downstream source code-related classification tasks (i.e., code smell classification, code language classification, technical debt classification, and code comment classification). After compared with state-of-the-art baselines for these tasks, EL-CodeBert can achieve better performance in most performance measures. Finally, we also conduct ablation studies to verify the rationality of the component setting in our proposed approach.4d:T934,"])</script><script>self.__next_f.push([1,"@inproceedings{10.1145/3545258.3545260,\n  author = {Liu, Ke and Yang, Guang and Chen, Xiang and Zhou, Yanlin},\n  title = {EL-CodeBert: Better Exploiting CodeBert to Support Source Code-Related Classification Tasks},\n  abbr = {Internetware'22},\n  tags = {EI; CCF-C},\n  year = {2022},\n  isbn = {9781450397803},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  url = {https://doi.org/10.1145/3545258.3545260},\n  doi = {10.1145/3545258.3545260},\n  abstract = {With the development of deep learning and natural language processing techniques, the performance of many source code-related tasks can be improved by using pre-trained models. Of these pre-trained models, CodeBert is a bi-modal pre-trained model for programming languages and natural languages, which has been successfully used in current source code-related tasks. These previous studies mainly use the output vector of CodeBertâ€™s last layer as the code semantic representation for fine-tuning downstream source code-related tasks. However, this setting may miss the valuable representational information, which may be captured by other layers of CodeBert. To better exploit the representational information in each layer of CodeBert for fine-tuning downstream source code-related tasks, we propose an approach EL-CodeBert. Our approach first extracts the representational information in each layer of CodeBert and views them as a representational information sequence. Then our approach learns the importance of representational information in each layer through the bidirectional recurrent neural network (i.e., Bi-LSTM) and the attention mechanism. To verify the effectiveness of our proposed approach, we select four downstream source code-related classification tasks (i.e., code smell classification, code language classification, technical debt classification, and code comment classification). After compared with state-of-the-art baselines for these tasks, EL-CodeBert can achieve better performance in most performance measures. Finally, we also conduct ablation studies to verify the rationality of the component setting in our proposed approach.},\n  booktitle = {Proceedings of the 13th Asia-Pacific Symposium on Internetware},\n  pages = {147â€“155},\n  numpages = {9},\n  location = {Hohhot, China},\n  series = {Internetware '22},\n  month = {9}\n}"])</script><script>self.__next_f.push([1,"4e:T650,On Stack Overflow, developers can not only browse question posts to solve their programming problems but also gain expertise from the question posts to help improve their programming skills. Therefore, improving the quality of question posts in Stack Overflow has attracted the wide attention of researchers. A concise and precise title can play an important role in helping developers understand the key information of the question post, which can improve the post quality. How-ever, the quality of the generated title is not high due to the lack of professional knowledge related to their questions or the poor presentation ability of developers. A previous study aimed to automatically generate the title by analyzing the code snippets in the question post. However, this study ignored the useful information in the corresponding problem description. Therefore, we propose an approach SOTitle for automatic post title generation by leveraging the code snippets and the problem description in the question post (i.e., the multi-modal input). SOTitle follows the Transformer structure, which can effectively capture long-term dependencies through a multi-head attention mechanism. To verify the effectiveness of SOTitle, we construct a large-scale high-quality corpus from Stack Overflow, which includes 1,168,257 high-quality question posts for four popular programming languages. Experimental results show that SOTitle can significantly outperform six state-of-the-art baselines in both automatic evaluation and human evaluation. To encourage follow-up studies, we make our corpus and approach publicly available.4f:T83e,"])</script><script>self.__next_f.push([1,"@INPROCEEDINGS{9825881,\n  author = {Liu, Ke and Yang, Guang and Chen, Xiang and Yu, Chi},\n  booktitle = {2022 IEEE International Conference on Software Analysis, Evolution and Reengineering},\n  abbr = {SANER'22},\n  tags = {EI; CCF-B},\n  title = {SOTitle: A Transformer-based Post Title Generation Approach for Stack Overflow},\n  year = {2022},\n  volume = {},\n  number = {},\n  pages = {577-588},\n  abstract = {On Stack Overflow, developers can not only browse question posts to solve their programming problems but also gain expertise from the question posts to help improve their programming skills. Therefore, improving the quality of question posts in Stack Overflow has attracted the wide attention of researchers. A concise and precise title can play an important role in helping developers understand the key information of the question post, which can improve the post quality. How-ever, the quality of the generated title is not high due to the lack of professional knowledge related to their questions or the poor presentation ability of developers. A previous study aimed to automatically generate the title by analyzing the code snippets in the question post. However, this study ignored the useful information in the corresponding problem description. Therefore, we propose an approach SOTitle for automatic post title generation by leveraging the code snippets and the problem description in the question post (i.e., the multi-modal input). SOTitle follows the Transformer structure, which can effectively capture long-term dependencies through a multi-head attention mechanism. To verify the effectiveness of SOTitle, we construct a large-scale high-quality corpus from Stack Overflow, which includes 1,168,257 high-quality question posts for four popular programming languages. Experimental results show that SOTitle can significantly outperform six state-of-the-art baselines in both automatic evaluation and human evaluation. To encourage follow-up studies, we make our corpus and approach publicly available.},\n  doi = {10.1109/SANER53432.2022.00075},\n  ISSN = {1534-5351},\n  month = {March}\n}"])</script><script>self.__next_f.push([1,"50:T693,A shellcode is a small piece of code and it is executed to exploit a software vulnerability, which allows the target computer to execute arbitrary commands from the attacker through a code injection attack. Similar to the purpose of automated vulnerability generation techniques, the automated generation of shellcode can generate attack instructions, which can be used to detect vulnerabilities and implement defensive measures. While the automated summarization of shellcode can help users unfamiliar with shellcode and network information security understand the intent of shellcode attacks. In this study, we propose a novel approach DualSC to solve the automatic shellcode generation and summarization tasks. Specifically, we formalize automatic shellcode generation and summarization as dual tasks, use a shallow Transformer for model construction, and design a normalization method Adjust_QKNorm to adapt these low-resource tasks (i.e., insufficient training data). Finally, to alleviate the out-of-vocabulary problem, we propose a rule-based repair component to improve the performance of automatic shellcode generation. In our empirical study, we select a high-quality corpus Shellcode_IA32 as our empirical subject. This corpus was gathered from two real-world projects based on the line-by-line granularity. We first compare DualSC with six state-of-the-art baselines from the code generation and code summarization domains in terms of four performance measures. The comparison results show the competitiveness of DualSC. Then, we verify the effectiveness of the component setting in DualSC. Finally, we conduct a human study to further verify the effectiveness of DualSC.51:T895,"])</script><script>self.__next_f.push([1,"@INPROCEEDINGS{9825869,\n  author = {Yang, Guang and Chen, Xiang and Zhou, Yanlin and Yu, Chi},\n  booktitle = {2022 IEEE International Conference on Software Analysis, Evolution and Reengineering},\n  abbr = {SANER'22},\n  tags = {EI; CCF-B},\n  title = {DualSC: Automatic Generation and Summarization of Shellcode via Transformer and Dual Learning},\n  year = {2022},\n  volume = {},\n  number = {},\n  pages = {361-372},\n  abstract = {A shellcode is a small piece of code and it is executed to exploit a software vulnerability, which allows the target computer to execute arbitrary commands from the attacker through a code injection attack. Similar to the purpose of automated vulnerability generation techniques, the automated generation of shellcode can generate attack instructions, which can be used to detect vulnerabilities and implement defensive measures. While the automated summarization of shellcode can help users unfamiliar with shellcode and network information security understand the intent of shellcode attacks. In this study, we propose a novel approach DualSC to solve the automatic shellcode generation and summarization tasks. Specifically, we formalize automatic shellcode generation and summarization as dual tasks, use a shallow Transformer for model construction, and design a normalization method Adjust_QKNorm to adapt these low-resource tasks (i.e., insufficient training data). Finally, to alleviate the out-of-vocabulary problem, we propose a rule-based repair component to improve the performance of automatic shellcode generation. In our empirical study, we select a high-quality corpus Shellcode_IA32 as our empirical subject. This corpus was gathered from two real-world projects based on the line-by-line granularity. We first compare DualSC with six state-of-the-art baselines from the code generation and code summarization domains in terms of four performance measures. The comparison results show the competitiveness of DualSC. Then, we verify the effectiveness of the component setting in DualSC. Finally, we conduct a human study to further verify the effectiveness of DualSC.},\n  doi = {10.1109/SANER53432.2022.00052},\n  ISSN = {1534-5351},\n  month = {March}\n}"])</script><script>self.__next_f.push([1,"52:T673,A smart contract is a computer program, which is intended to automatically execute, control or document legally relevant events and actions according to the terms of a contract. About 10% of the security vulnerabilities in smart contracts are caused by misuse of codes without comments. Therefore, there is a need to design effective automatic code comment generation methods for smart contracts. In this study, we propose an information retrieval-based code comment generation method CCGIR for smart contracts. Since code clones are common in smart contract development, CCGIR finds the most similar code in the code repository and reuses its comment through an information retrieval approach from three aspects: semantic similarity, lexical similarity, and syntactic similarity of smart contract codes. We select a corpus, which contains 57,676 unique pairs of \u003cmethod, comment\u003e from 40,932 real-world smart contracts, as our experimental subject. Then we conduct empirical studies to evaluate the effectiveness of our proposed method. Experimental results show that CCGIR can outperform nine state-of-the-art baselines in terms of three performance measures. Moreover, we perform a human study to further verify that CCGIR can generate higher quality comments. Finally, we find CCGIR can achieve promising performance on the other two code comment generation tasks (i.e., code comment generation for Java and code comment generation for Python). Due to the simplicity and effectiveness of our proposed method, we recommend researchers can use our proposed method as the baseline when evaluating their proposed novel code comment generation methods.53:T891,"])</script><script>self.__next_f.push([1,"@article{YANG2022107858,\n  title = {CCGIR: Information retrieval-based code comment generation method for smart contracts},\n  abbr = {KBS'22},\n  tags = {SCI-Q1; CCF-C},\n  journal = {Knowledge-Based Systems},\n  volume = {237},\n  pages = {107858},\n  year = {2022},\n  month = {2},\n  issn = {0950-7051},\n  doi = {https://doi.org/10.1016/j.knosys.2021.107858},\n  url = {https://www.sciencedirect.com/science/article/pii/S0950705121010406},\n  author = {Guang Yang and Ke Liu and Xiang Chen and Yanlin Zhou and Chi Yu and Hao Lin},\n  abstract = {A smart contract is a computer program, which is intended to automatically execute, control or document legally relevant events and actions according to the terms of a contract. About 10% of the security vulnerabilities in smart contracts are caused by misuse of codes without comments. Therefore, there is a need to design effective automatic code comment generation methods for smart contracts. In this study, we propose an information retrieval-based code comment generation method CCGIR for smart contracts. Since code clones are common in smart contract development, CCGIR finds the most similar code in the code repository and reuses its comment through an information retrieval approach from three aspects: semantic similarity, lexical similarity, and syntactic similarity of smart contract codes. We select a corpus, which contains 57,676 unique pairs of \u003cmethod, comment\u003e from 40,932 real-world smart contracts, as our experimental subject. Then we conduct empirical studies to evaluate the effectiveness of our proposed method. Experimental results show that CCGIR can outperform nine state-of-the-art baselines in terms of three performance measures. Moreover, we perform a human study to further verify that CCGIR can generate higher quality comments. Finally, we find CCGIR can achieve promising performance on the other two code comment generation tasks (i.e., code comment generation for Java and code comment generation for Python). Due to the simplicity and effectiveness of our proposed method, we recommend researchers can use our proposed method as the baseline when evaluating their proposed novel code comment generation methods.}\n}"])</script><script>self.__next_f.push([1,"54:T42f,Pseudo-code written by natural language is helpful for novice developers' program comprehension. However, writing such pseudo-code is time-consuming and laborious. Motivated by the research advancements of sequence-to-sequence learning and code semantic learning, we propose a novel deep pseudo-code generation method DeepPseudo via code feature extraction and Transformer. In particular, DeepPseudo utilizes a Transformer encoder to perform encoding for source code and then use a code feature extractor to learn the knowledge of local features. Finally, it uses a pseudo-code generator to perform decoding, which can generate the corresponding pseudo-code. We choose two corpora (i.e., Django and SPoC) from real-world large-scale projects as our empirical subjects. We first compare DeepPseudo with seven state-of-the-art baselines from pseudo-code generation and neural machine translation domains in terms of four performance measures. Results show the competitiveness of DeepPseudo. Moreover, we also analyze the rationality of the component settings in DeepPseudo.55:T5fa,@INPROCEEDINGS{9712127,\n  author = {Yang, Guang and Zhou, Yanlin and Chen, Xiang and Yu, Chi},\n  booktitle = {Proceedings of the 28th Asia-Pacific Software Engineering Conference},\n  title = {Fine-grained Pseudo-code Generation Method via Code Feature Extraction and Transformer},\n  abbr = {APSEC'21},\n  tags = {EI; CCF-C},\n  year = {2021},\n  month = {Dec},\n  pages = {213-222},\n  abstract = {Pseudo-code written by natural language is helpful for novice developers' program comprehension. However, writing such pseudo-code is time-consuming and laborious. Motivated by the research advancements of sequence-to-sequence learning and code semantic learning, we propose a novel deep pseudo-code generation method DeepPseudo via code feature extraction and Transformer. In particular, DeepPseudo utilizes a Transformer encoder to perform encoding for source code and then use a code feature extractor to learn the knowledge of local features. Finally, it uses a ps"])</script><script>self.__next_f.push([1,"eudo-code generator to perform decoding, which can generate the corresponding pseudo-code. We choose two corpora (i.e., Django and SPoC) from real-world large-scale projects as our empirical subjects. We first compare DeepPseudo with seven state-of-the-art baselines from pseudo-code generation and neural machine translation domains in terms of four performance measures. Results show the competitiveness of DeepPseudo. Moreover, we also analyze the rationality of the component settings in DeepPseudo.},\n  doi = {10.1109/APSEC53868.2021.00029},\n  ISSN = {2640-0715}\n}56:T420,Bug severity is important for triagers. Recently, the text in the summary field (i.e., bug summary) of bug reports is usually used to extract features, and then bug report severity prediction models are constructed. In some bug reports, the bug summary may not contain enough useful information. While the text field in the description (i.e., bug description) of bug reports contains detailed information of the bug (e.g., steps to reproduce the bug, stack traces, and expected behavior). However, the bug description may contain irrelevant information. Motivated by the above findings, we propose a novel method EKD-BSP (Bug Report Severity Prediction by Extracting Keywords from Description), which uses the bug summary and the keywords extracted from the bug description to perform severity prediction. Our empirical study selects two large-scale open-source projects (i.e., Eclipse and Mozilla) as the empirical subjects. The empirical results show that EKD-BSP can improve the performance of F-measure by up to 5.19% after compared with the baselines.57:T602,@INPROCEEDINGS{9623014,\n  author = {Jia, Yanxin and Chen, Xiang and Xu, Shuyuan and Yang, Guang and Cao, Jinxin},\n  booktitle = {Proceedings of the 8th International Conference on Dependable Systems and Their Applications},\n  abbr = {DSA'21},\n  tags = {EI},\n  title = {EKD-BSP: Bug Report Severity Prediction by Extracting Keywords from Description},\n  year = {2021},\n  pages = {42-53},\n  abstract = {Bug se"])</script><script>self.__next_f.push([1,"verity is important for triagers. Recently, the text in the summary field (i.e., bug summary) of bug reports is usually used to extract features, and then bug report severity prediction models are constructed. In some bug reports, the bug summary may not contain enough useful information. While the text field in the description (i.e., bug description) of bug reports contains detailed information of the bug (e.g., steps to reproduce the bug, stack traces, and expected behavior). However, the bug description may contain irrelevant information. Motivated by the above findings, we propose a novel method EKD-BSP (Bug Report Severity Prediction by Extracting Keywords from Description), which uses the bug summary and the keywords extracted from the bug description to perform severity prediction. Our empirical study selects two large-scale open-source projects (i.e., Eclipse and Mozilla) as the empirical subjects. The empirical results show that EKD-BSP can improve the performance of F-measure by up to 5.19% after compared with the baselines.},\n  doi = {10.1109/DSA52907.2021.00014},\n  ISSN = {2767-6684},\n  month = {Aug}\n}58:T42b,Developers often write low-quality code comments due to the lack of programming experience, which can reduce the efficiency of developers' program comprehension. Therefore, developers hope that code comment generation tools can be developed to illustrate the functionality and purpose of the code. Recently, researchers mainly model this problem as the neural machine translation problem and tend to use deep learning-based methods. In this study, we propose a novel method ComFormer based on Transformer and fusion method-based hybrid code presentation. Moreover, to alleviate OOV (out-of-vocabulary) problem and speed up model training, we further utilize the Byte-BPE algorithm to split identifiers and Sim_SBT method to perform AST Traversal. We compare ComFormer with seven state-of-the-art baselines from code comment generation and neural machine translation domains. Comparison results show the compet"])</script><script>self.__next_f.push([1,"itiveness of ComFormer in terms of three performance measures. Moreover, we perform a human study to verify that ComFormer can generate high-quality comments.59:T63b,@INPROCEEDINGS{9622968,\n  author = {Yang, Guang and Chen, Xiang and Cao, Jinxin and Xu, Shuyuan and Cui, Zhanqi and Yu, Chi and Liu, Ke},\n  booktitle = {Proceedings of the 8th International Conference on Dependable Systems and Their Applications},\n  abbr = {DSA'21},\n  tags = {EI},\n  title = {ComFormer: Code Comment Generation via Transformer and Fusion Method-based Hybrid Code Representation},\n  year = {2021},\n  pages = {30-41},\n  abstract = {Developers often write low-quality code comments due to the lack of programming experience, which can reduce the efficiency of developers' program comprehension. Therefore, developers hope that code comment generation tools can be developed to illustrate the functionality and purpose of the code. Recently, researchers mainly model this problem as the neural machine translation problem and tend to use deep learning-based methods. In this study, we propose a novel method ComFormer based on Transformer and fusion method-based hybrid code presentation. Moreover, to alleviate OOV (out-of-vocabulary) problem and speed up model training, we further utilize the Byte-BPE algorithm to split identifiers and Sim_SBT method to perform AST Traversal. We compare ComFormer with seven state-of-the-art baselines from code comment generation and neural machine translation domains. Comparison results show the competitiveness of ComFormer in terms of three performance measures. Moreover, we perform a human study to verify that ComFormer can generate high-quality comments.},\n  doi = {10.1109/DSA52907.2021.00013},\n  ISSN = {2767-6684},\n  month = {Aug}\n}5a:T579,@inproceedings{DBLP:conf/seke/YangZYC21,\n  author = {Guang Yang and\r\n                  Yanlin Zhou and\r\n                  Chi Yu and\r\n                  Xiang Chen},\n  abbr = {SEKE'21},\n  tags = {EI; CCF-C},\n  editor = {Shi{-}Kuo Chang},\n  title = {DeepSCC: Source Code Classifica"])</script><script>self.__next_f.push([1,"tion Based on Fine-Tuned RoBERTa {(S)}},\n  booktitle = {Proceedings of the 33rd International Conference on Software Engineering and Knowledge Engineering},\n  pages = {499--502},\n  publisher = {{KSI} Research Inc.},\n  year = {2021},\n  month = {7},\n  url = {https://doi.org/10.18293/SEKE2021-005},\n  doi = {10.18293/SEKE2021-005},\n  abstract = {In software engineering-related tasks (such as programming language tag prediction based on code snippets from Stack Overflow), the programming language classification for code snippets is a common task. In this study, we propose a novel method DeepSCC, which uses a fine-tuned RoBERTa model to classify the programming language type of the source code. In our empirical study, we choose a corpus collected from Stack Overflow, which contains 224,445 pairs of code snippets and corresponding language types. After comparing nine state-of-the-art baselines from the fields of source code classification and neural text classification in terms of four performance measures (i.e., Accuracy, Precision, Recall, and F1), we show the competitiveness of our proposed method DeepSCC.}\n}5b:T4f2,@article{Yang2021deep,\n  title = {åŸºäºŽæ·±åº¦å­¦ä¹ çš„ Stack Overflow é—®é¢˜å¸–åˆ†ç±»æ–¹æ³•},\n  abbr = {å‰æž—å¤§å­¦å­¦æŠ¥ (ç†å­¦ç‰ˆ)'21},\n  tags = {åŒ—æ ¸},\n  author = {Guang Yang and Yanxin Jia and Xiang Chen and Shuyuan Xu},\n  journal = {å‰æž—å¤§å­¦å­¦æŠ¥ (ç†å­¦ç‰ˆ)},\n  volume = {59},\n  number = {4},\n  pages = {922--928},\n  year = {2021},\n  abstract = {The classification methods based on regular expressions and traditional machine learning had the problems of manual extraction of patterns and performance bottleneck, we proposed deep learning-based classification methods for question post, the deep text mining model TextCNN and integrating attention mechanismâ€”TextRNN were used to construct a classification model. The experimental results show that the classification performance of deep learning-based methods is better than the existing benchmark methods on most of the question purpose categories, "])</script><script>self.__next_f.push([1,"and the Adam optimizer is better than the SGD optimizer, and the Glove pre-trained word vector is better than randomly generated word vectors. The method classifies posts for the purpose of asking question, which can add a new dimension to the analysis of post discussion topics on Stack Overflow (SO).},\n  doi = {10.13413/j.cnki.jdxblxb.2020165}\n}5c:T58d,@article{RJXB202107011,\n  author = {Xiang Chen and Guang Yang and Zhanqi Cui and Guozhu Meng and Zan Wang },\n  title = {ä»£ç æ³¨é‡Šè‡ªåŠ¨ç”Ÿæˆæ–¹æ³•ç»¼è¿°},\n  abbr = {è½¯ä»¶å­¦æŠ¥'21},\n  tags = {EI; ä¸­æ–‡CCF-A; åŒ—æ ¸},\n  journal = {è½¯ä»¶å­¦æŠ¥},\n  volume = {32},\n  number = {07},\n  pages = {2118-2141},\n  year = {2021},\n  issn = {1000-9825},\n  doi = {10.13328/j.cnki.jos.006258},\n  abstract = {During software development and maintenance, code comments often have some problems, such as missing, insufficient, or mismatching with code content. Writing high-quality code comments takes time and effort for developers, and the quality can not be guaranteed, therefore, it is urgent for researchers to design effective automatic code comment generation methods. The automatic code comment generation issue is an active research topic in the program comprehension domain. This study conducts a systematic review of this research topic. The existing methods are divided into three categories:Template-based generation methods, information retrieval-based methods, and deep learning-based methods. Related studies are analyzed and summarizedfor each category. Then, the corpora and comment quality evaluation methods that are often used in previous studiesare analyzed, which can facilitate the experimental study for future studies. Finally, the potential research directions in the future aresummarized and discussed.}\n}"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12\",\"children\":[[\"$\",\"$L15\",null,{\"config\":{\"type\":\"publication\",\"title\":\"Publications\",\"source\":\"publications.bib\"},\"publications\":[{\"id\":\"gao2025resource\",\"title\":\"Resource-efficient automatic software vulnerability assessment via knowledge distillation and particle swarm optimization\",\"abbr\":\"EAAI'25\",\"authors\":[{\"name\":\"Chaoyang Gao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Jiyu Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jibin Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"11\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-C\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"Engineering Applications of Artificial Intelligence\",\"conference\":\"\",\"volume\":\"160\",\"pages\":\"111914\",\"doi\":\"10.1016/j.engappai.2025.111914\",\"code\":\"https://github.com/judeomg/PSO-KDVA\",\"abstract\":\"$16\",\"description\":\"A resource-efficient framework integrating particle swarm optimization and knowledge distillation.\",\"selected\":false,\"bibtex\":\"$17\"},{\"id\":\"yang2025large\",\"title\":\"Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead\",\"abbr\":\"Preprints'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wei Zheng\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dong Liang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Peng Hu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yukui Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shaohua Peng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenghan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiahui Feng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiao Wei\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Kexin Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Deyuan Ma\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haotian Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiheng Shen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xing Hu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Terry Yue Zhuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Lo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"11\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Preprint\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"Preprints\",\"conference\":\"\",\"doi\":\"10.20944/preprints202511.0656.v2\",\"abstract\":\"$18\",\"description\":\"A comprehensive literature review of LLMs for Verilog code generation, highlighting their strengths, limitations, and potential applications.\",\"selected\":false,\"bibtex\":\"$19\"},{\"id\":\"liu2025evaluating\",\"title\":\"Evaluating and Improving Framework-based Parallel Code Completion with Large Language Models\",\"abbr\":\"ASE'25\",\"authors\":[{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qinglin Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"YiGui Feng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Gencheng Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jie Liu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"11\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"CCF-A\",\"EI\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering\",\"abstract\":\"$1a\",\"description\":\"A novel framework-based parallel code completion method for code generation, balancing accuracy, efficiency and explainability.\",\"selected\":false,\"bibtex\":\"$1b\"},{\"id\":\"yang2025codediting\",\"title\":\"Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases\",\"abbr\":\"ASE'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wei Zheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xing Hu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Lo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"11\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"CCF-A\",\"EI\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering\",\"code\":\"https://github.com/CODE-DITING/CODE-DITING\",\"abstract\":\"$1c\",\"description\":\"A novel LLM-as-Judge method for code evaluation, balancing accuracy, efficiency and explainability.\",\"selected\":true,\"bibtex\":\"$1d\"},{\"id\":\"zhou2025sejury\",\"title\":\"SE-Jury: An LLM-as-Ensemble-Judge Metric for Narrowing the Gap with Human Evaluation in SE\",\"abbr\":\"ASE'25\",\"authors\":[{\"name\":\"Xin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Kisub Kim\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ting Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Martin Weyssow\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Luis F. Gomes\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Kui Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Xia\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Lo\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"11\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"CCF-A\",\"EI\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering\",\"code\":\"https://github.com/Xin-Zhou-smu/LLMJudge\",\"abstract\":\"$1e\",\"description\":\"A novel LLM-as-Ensemble-Judge metric for SE, balancing accuracy, efficiency and explainability.\",\"selected\":false,\"bibtex\":\"$1f\"},{\"id\":\"yang2025cream\",\"title\":\"The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation\",\"abbr\":\"arXiv'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wei Zheng\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yifan Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fengji Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Terry Yue Zhuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"9\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Preprint\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"arXiv\",\"conference\":\"\",\"doi\":\"10.48550/arXiv.2509.20215\",\"abstract\":\"LLMs face significant challenges in Verilog generation due to limited domain-specific knowledge. While sampling techniques improve pass@k metrics, hardware engineers need one trustworthy solution rather than uncertain candidates. To bridge this gap, we formulate it as a semantic alignment problem between requirements and Verilog implementations, and propose VCD-RNK, a discriminator model tailored for efficient Verilog code reranking. Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling expert knowledge across three dimensions: code semantic analysis, test case generation, and functional correctness assessment. By explicitly simulating the above reasoning processes during inference, VCD-RNK effectively avoids computationally intensive test execution in existing methods.\",\"description\":\"A novel reranking method for Verilog code generation, highlighting their strengths, limitations, and potential applications.\",\"selected\":false,\"bibtex\":\"$20\"},{\"id\":\"zhang2025beyond\",\"title\":\"Beyond Sequences: Two-dimensional Representation and Dependency Encoding for Code Generation\",\"abbr\":\"ACL'25\",\"authors\":[{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wei Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"CCF-A\",\"EI\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics\",\"pages\":\"6157--6172\",\"doi\":\"10.18653/v1/2025.acl-long.308\",\"abstract\":\"$21\",\"description\":\"A novel dependency encoding approach for code generation, highlighting its generalizability, context understanding and retrieval, as well as interpretability in code generation.\",\"selected\":false,\"bibtex\":\"$22\"},{\"id\":\"11005718\",\"title\":\"Anchor Attention, Small Cache: Code Generation With Large Language Models\",\"abbr\":\"TSE'25\",\"authors\":[{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Harald C. Gall\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-A\"],\"keywords\":[],\"researchArea\":\"transformer-architectures\",\"journal\":\"IEEE Transactions on Software Engineering\",\"conference\":\"\",\"volume\":\"51\",\"issue\":\"6\",\"pages\":\"1866-1881\",\"doi\":\"10.1109/TSE.2025.3570680\",\"code\":\"https://github.com/NUAAZXY/Anchor_Coder\",\"abstract\":\"$23\",\"description\":\"A novel attention mechanism and cache mechanism for code generation with LLMs, achieving significant reduction in KV cache requirements while preserving the majority of model's performance.\",\"selected\":false,\"bibtex\":\"$24\"},{\"id\":\"10.1145/3735636\",\"title\":\"Less is More: DocString Compression in Code Generation\",\"abbr\":\"TOSEM'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Wei Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Terry Yue Zhuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Lo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-A\"],\"keywords\":[\"DocString Compression\",\"Code Generation\",\"Large Language Model\"],\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Software Engineering and Methodology\",\"conference\":\"\",\"doi\":\"10.1145/3735636\",\"code\":\"https://github.com/NTDXYG/ShortenDoc\",\"abstract\":\"$25\",\"description\":\"A novel DocString compression method for code generation, achieving significant reduction in token processing cost while preserving the quality of the generated code.\",\"selected\":true,\"bibtex\":\"$26\"},{\"id\":\"YANG2025107699\",\"title\":\"Assessing and improving syntactic adversarial robustness of pre-trained models for code translation\",\"abbr\":\"IST'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tingting Han\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q2\",\"CCF-B\"],\"keywords\":[\"Code translation\",\"Adversarial robustness\",\"Pre-trained models\",\"Data augmentation\",\"Adversarial training\"],\"researchArea\":\"machine-learning\",\"journal\":\"Information and Software Technology\",\"conference\":\"\",\"volume\":\"181\",\"pages\":\"107699\",\"doi\":\"https://doi.org/10.1016/j.infsof.2025.107699\",\"url\":\"https://www.sciencedirect.com/science/article/pii/S0950584925000382\",\"code\":\"https://github.com/NTDXYG/CoTR\",\"abstract\":\"$27\",\"description\":\"A novel approach to assess and improve the syntactic adversarial robustness of PTMs in code translation, achieving significant reduction in syntactic adversarial robustness while preserving the majority of model's performance.\",\"selected\":false,\"bibtex\":\"$28\"},{\"id\":\"10.1145/3728639\",\"title\":\"Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss\",\"abbr\":\"TOSEM'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Terry Zhuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Lo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-A\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Software Engineering and Methodology\",\"conference\":\"\",\"doi\":\"10.1145/3728639\",\"url\":\"https://doi.org/10.1145/3728639\",\"code\":\"https://github.com/NTDXYG/DeCE\",\"abstract\":\"$29\",\"description\":\"A general and effective loss function DeCE (Deceptive Cross-Entropy) to defend Code Language Models against backdoor attacks, preventing overfitting to backdoor triggers.\",\"selected\":true,\"bibtex\":\"$2a\"},{\"id\":\"yang2025moregreencodelarge\",\"title\":\"Less is More: Towards Green Code Large Language Models via Unified Structural Pruning\",\"abbr\":\"arXiv'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wei Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Terry Yue Zhuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"4\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Preprint\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"arXiv\",\"conference\":\"\",\"doi\":\"10.48550/arXiv.2412.15921\",\"url\":\"https://arxiv.org/abs/2412.15921\",\"code\":\"https://github.com/Flab-Pruner/Flab-Pruner\",\"abstract\":\"$2b\",\"description\":\"A novel unified structural pruning method for Code Large Language Models, achieving significant reduction in computational demands and energy consumption while preserving the majority of model's performance.\",\"selected\":false,\"bibtex\":\"$2c\"},{\"id\":\"10928415\",\"title\":\"RegexExplainer: Automatic Description Generation for Regular Expressions via Transformer\",\"abbr\":\"ICCTIT'24\",\"authors\":[{\"name\":\"Yun Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuangbo Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tianyue Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiao Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\"],\"keywords\":[\"Computer languages;Operating systems;Neural machine translation;Pressing;Syntactics;Lead;Transformers;Encoding;Communications technology;Information technology;Program Comprehension;Regular Expression;Transformer\"],\"researchArea\":\"neural-networks\",\"journal\":\"\",\"conference\":\"Proceedings of the 4th International Conference on Communication Technology and Information Technology\",\"volume\":\"\",\"issue\":\"\",\"pages\":\"364-368\",\"doi\":\"10.1109/ICCTIT64404.2024.10928415\",\"abstract\":\"$2d\",\"description\":\"A novel method to automatically generate functional descriptions for regular expressions, achieving significant improvement in performance compared to the state-of-the-art baselines.\",\"selected\":false,\"bibtex\":\"$2e\"},{\"id\":\"10634302\",\"title\":\"Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models\",\"abbr\":\"TSE'24\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Terry Yue Zhuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"12\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-A\"],\"keywords\":[\"Codes;Cotton;Task analysis;Computational modeling;Benchmark testing;Training;Software engineering;Code generation;chain-of-thought;large language model;lightweight language model;program language processing\"],\"researchArea\":\"signal-processing\",\"journal\":\"IEEE Transactions on Software Engineering\",\"conference\":\"\",\"volume\":\"50\",\"issue\":\"9\",\"pages\":\"2437-2457\",\"doi\":\"10.1109/TSE.2024.3440503\",\"code\":\"https://github.com/NTDXYG/COTTON\",\"abstract\":\"$2f\",\"description\":\"A novel approach COTTON to automatically generate CoTs for code generation, achieving significant improvement in performance compared to the state-of-the-art baselines.\",\"selected\":true,\"bibtex\":\"$30\"},{\"id\":\"ZHANG2024112066\",\"title\":\"Context-aware code generation with synchronous bidirectional decoder\",\"abbr\":\"JSS'24\",\"authors\":[{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tingting Han\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"8\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q2\",\"CCF-B\"],\"keywords\":[\"Code generation\",\"Transition system\",\"Bidirectional decoder\",\"Neural network\"],\"researchArea\":\"neural-networks\",\"journal\":\"Journal of Systems and Software\",\"conference\":\"\",\"volume\":\"214\",\"pages\":\"112066\",\"doi\":\"https://doi.org/10.1016/j.jss.2024.112066\",\"url\":\"https://www.sciencedirect.com/science/article/pii/S0164121224001110\",\"abstract\":\"$31\",\"description\":\"A novel context-sensitive model employing a bidirectional decoder to generate tokens in two different orders synchronously and interactively.\",\"selected\":false,\"bibtex\":\"$32\"},{\"id\":\"yang2024automatic\",\"title\":\"Automatic bi-modal question title generation for Stack Overflow with prompt learning\",\"abbr\":\"EMSE'24\",\"authors\":[{\"name\":\"Shaoyu Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"5\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q2\",\"CCF-B\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"Empirical Software Engineering\",\"conference\":\"\",\"volume\":\"29\",\"issue\":\"3\",\"pages\":\"63\",\"doi\":\"10.1007/s10664-024-10466-4\",\"code\":\"https://github.com/shaoyuyoung/SOTitlePlus\",\"abstract\":\"$33\",\"description\":\"A novel approach SOTitle+ to automatically generate the titles for Stack Overflow question posts.\",\"selected\":false,\"bibtex\":\"$34\"},{\"id\":\"zhao2024automatic\",\"title\":\"Automatic smart contract comment generation via large language models and in-context learning\",\"abbr\":\"IST'24\",\"authors\":[{\"name\":\"Junjie Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiheng Shen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"4\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q2\",\"CCF-B\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"Information and Software Technology\",\"conference\":\"\",\"volume\":\"168\",\"pages\":\"107405\",\"doi\":\"10.1016/j.infsof.2024.107405\",\"code\":\"https://github.com/jun-jie-zhao/SCCLLM\",\"abstract\":\"$35\",\"description\":\"A novel approach SCCLLM to automatically generate the comments for smart contract code.\",\"selected\":false,\"bibtex\":\"$36\"},{\"id\":\"shen2024bash\",\"title\":\"Bash comment generation via data augmentation and semantic-aware CodeBERT\",\"abbr\":\"ASEJ'24\",\"authors\":[{\"name\":\"Yiheng Shen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaolin Ju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"3\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q2\",\"CCF-B\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"Automated Software Engineering\",\"conference\":\"\",\"volume\":\"31\",\"issue\":\"1\",\"pages\":\"30\",\"doi\":\"10.1007/s10515-024-00431-2\",\"code\":\"https://github.com/syhstudy/Bash2Com\",\"abstract\":\"$37\",\"description\":\"A novel two-module method named Bash2Com for Bash code comments generation.\",\"selected\":false,\"bibtex\":\"$38\"},{\"id\":\"yang2024important\",\"title\":\"How important are good method names in neural code generation? a model robustness perspective\",\"abbr\":\"TOSEM'24\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wenhua Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tao Yue\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"3\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-A\"],\"keywords\":[],\"researchArea\":\"neural-networks\",\"journal\":\"ACM Transactions on Software Engineering and Methodology\",\"conference\":\"\",\"volume\":\"33\",\"issue\":\"3\",\"pages\":\"1--35\",\"doi\":\"10.1145/3630010\",\"code\":\"https://github.com/NTDXYG/RADAR\",\"abstract\":\"$39\",\"description\":\"A novel approach RADAR to enhance the performance of PCGMs from a model robustness perspective.\",\"selected\":true,\"bibtex\":\"$3a\"},{\"id\":\"2023-30715\",\"title\":\"CodeScore-Rï¼šç”¨äºŽè¯„ä¼°ä»£ç åˆæˆåŠŸèƒ½å‡†ç¡®æ€§çš„è‡ªåŠ¨åŒ–é²æ£’æŒ‡æ ‡\",\"abbr\":\"è®¡ç®—æœºç ”ç©¶ä¸Žå‘å±•'24\",\"authors\":[{\"name\":\"Yang Guang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhou Yu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Chen Xiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhang Xiangyu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"2\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"EI\",\"ä¸­æ–‡CCF-A\",\"åŒ—æ ¸\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"è®¡ç®—æœºç ”ç©¶ä¸Žå‘å±•\",\"conference\":\"\",\"volume\":\"61\",\"issue\":\"2\",\"pages\":\"291-306\",\"doi\":\"10.7544/issn1000-1239.202330715\",\"url\":\"https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330715\",\"abstract\":\"$3b\",\"description\":\"A novel automated robust metric CodeScore-R for evaluating the functional accuracy of code synthesis, achieving significant improvement in performance compared to the state-of-the-art baselines.\",\"selected\":false,\"bibtex\":\"$3c\"},{\"id\":\"zhang-etal-2023-syntax\",\"title\":\"Syntax-Aware Retrieval Augmented Code Generation\",\"abbr\":\"EMNLP'23\",\"authors\":[{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"CCF-B\",\"EI\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Findings of the Association for Computational Linguistics: EMNLP\",\"pages\":\"1291--1302\",\"doi\":\"10.18653/v1/2023.findings-emnlp.90\",\"url\":\"https://aclanthology.org/2023.findings-emnlp.90/\",\"code\":\"https://github.com/NUAAZXY/kNN-TRANX\",\"abstract\":\"$3d\",\"description\":\"A novel token-level retrieval augmented code generation method $k$NN-TRANX.\",\"selected\":false,\"bibtex\":\"$3e\"},{\"id\":\"yang2023syntax\",\"title\":\"A syntax-guided multi-task learning approach for Turducken-style code generation\",\"abbr\":\"EMSE'23\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiran Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tingting Han\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"10\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q2\",\"CCF-B\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"Empirical Software Engineering\",\"conference\":\"\",\"volume\":\"28\",\"issue\":\"6\",\"pages\":\"141\",\"doi\":\"10.1007/s10664-023-10372-1\",\"code\":\"https://github.com/NTDXYG/TurduckenGen\",\"abstract\":\"$3f\",\"description\":\"A novel syntax-guided multi-task learning approach for Turducken-style code generation.\",\"selected\":false,\"bibtex\":\"$40\"},{\"id\":\"10196969\",\"title\":\"EDP-BGCNN: Effective Defect Prediction via BERT-based Graph Convolutional Neural Network\",\"abbr\":\"COMPSAC'23\",\"authors\":[{\"name\":\"Hao Shen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaolin Ju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"8\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\",\"CCF-C\"],\"keywords\":[\"Codes;Semantics;Bit error rate;Syntactics;Feature extraction;Software;Software reliability\"],\"researchArea\":\"reliability-engineering\",\"journal\":\"\",\"conference\":\"Proceedings of the 47th Annual Computers, Software, and Applications Conference\",\"pages\":\"850-859\",\"doi\":\"10.1109/COMPSAC57700.2023.00114\",\"abstract\":\"$41\",\"description\":\"A novel BERT-based Graph Convolutional Neural Network for effective defect prediction.\",\"selected\":false,\"bibtex\":\"$42\"},{\"id\":\"DBLP:conf/seke/ShenJ0Y23\",\"title\":\"An Empirical Study of Adversarial Training in Code Comment Generation\",\"abbr\":\"SEKE'23\",\"authors\":[{\"name\":\"Yiheng Shen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaolin Ju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\",\"CCF-C\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 35th International Conference on Software Engineering and Knowledge Engineering\",\"pages\":\"292--297\",\"doi\":\"10.18293/SEKE2023-108\",\"url\":\"https://doi.org/10.18293/SEKE2023-108\",\"abstract\":\"The code comment generation task is designed for developers to understand programs more quickly during development and maintenance. However, the existing automatic code comment generation models can not generate valuable comments for developers. It is necessary to explore a technology that can optimize the performance of code comment generation models without changing the model. We consider adversarial training as the experimental object, which can improve the robustness and generalization of the model. We present a large-scale study to experimentally validate the performance of gradient-based adversarial training methods in the code comment generation task. The results show that adversarial training can improve the model performance by generating adversarial examples without changing the model. Our empirical study can provide a new perspective for researchers to improve the performance of code comment generation models.\",\"description\":\"An empirical study of adversarial training in code comment generation.\",\"selected\":false,\"bibtex\":\"$43\"},{\"id\":\"Zhang2023CCGRA\",\"title\":\"CCGRA: Smart Contract Code Comment Generation with Retrieval-enhanced Approach\",\"abbr\":\"SEKE'23\",\"authors\":[{\"name\":\"Shizhan Chen Zhenhua Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\",\"CCF-C\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 35th International Conference on Software Engineering and Knowledge Engineering\",\"pages\":\"212--217\",\"doi\":\"10.18293/SEKE2023-090\",\"url\":\"https://doi.org/10.18293/SEKE2023-090\",\"code\":\"https://github.com/ZZHbible/CCGRA\",\"abstract\":\"Smart contracts are self-executing programs on the blockchain that are critical to a range of industries, including finance, supply chain management, and healthcare. However, comprehending smart contracts can be challenging due to a lack of effective comments in most user-defined code. To address this challenge, we propose a novel retrieval-enhanced approach CCGRA that leverages retrieval knowledge to generate high-quality comments for Solidity language code. Our approach carefully eliminates duplicated data and template data in the widely-used smart contract dataset to ensure a high-quality corpus. Extensive experiments and comprehensive analysis demonstrate the effectiveness applicability of our approach after being compared with eight state-of-the-art baselines. Finally, we conduct a human study and find the comment quality generated by our approach is better than baselines in terms of similarity, naturalness, and informativeness.\",\"description\":\"A novel retrieval-enhanced approach CCGRA that leverages retrieval knowledge to generate high-quality comments for Solidity language code.\",\"selected\":false,\"bibtex\":\"$44\"},{\"id\":\"20231310\",\"title\":\"åŸºäºŽåŒé‡ä¿¡æ¯æ£€ç´¢çš„Bashä»£ç æ³¨é‡Šç”Ÿæˆæ–¹æ³•\",\"abbr\":\"è½¯ä»¶å­¦æŠ¥'23\",\"authors\":[{\"name\":\"Chen Xiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuelian Pu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhanqi Cui\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"03\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"EI\",\"ä¸­æ–‡CCF-A\",\"åŒ—æ ¸\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"è½¯ä»¶å­¦æŠ¥\",\"conference\":\"\",\"volume\":\"34\",\"issue\":\"3\",\"pages\":\"1310\",\"doi\":\"10.13328/j.cnki.jos.006690\",\"code\":\"https://github.com/ExplainBash/explainbash\",\"abstract\":\"$45\",\"description\":\"A novel approach to automatically generate the comments for Bash code based on dual information retrieval.\",\"selected\":false,\"bibtex\":\"$46\"},{\"id\":\"YANG2023111577\",\"title\":\"ExploitGen: Template-augmented exploit code generation based on CodeBERT\",\"abbr\":\"JSS'23\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tingting Han\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"3\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q2\",\"CCF-B\"],\"keywords\":[\"Exploit code\",\"Code generation\",\"Template parser\",\"CodeBERT\",\"Neural network\"],\"researchArea\":\"neural-networks\",\"journal\":\"Journal of Systems and Software\",\"conference\":\"\",\"volume\":\"197\",\"pages\":\"111577\",\"doi\":\"https://doi.org/10.1016/j.jss.2022.111577\",\"url\":\"https://www.sciencedirect.com/science/article/pii/S0164121222002539\",\"code\":\"https://github.com/NTDXYG/ExploitGen\",\"abstract\":\"$47\",\"description\":\"A novel template-augmented exploit code generation approach ExploitGen based on CodeBERT.\",\"selected\":false,\"bibtex\":\"$48\"},{\"id\":\"9978203\",\"title\":\"BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT\",\"abbr\":\"ICSME'22\",\"authors\":[{\"name\":\"Chi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yanlin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"10\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\",\"CCF-B\"],\"keywords\":[\"Training;Software maintenance;Codes;Linux;Semantics;Process control;Maintenance engineering;Technological;Bash Command;Code comment generation;Deep learning;Information retrieval\"],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 2022 IEEE International Conference on Software Maintenance and Evolution\",\"volume\":\"\",\"issue\":\"\",\"pages\":\"82-93\",\"doi\":\"10.1109/ICSME55016.2022.00016\",\"code\":\"https://github.com/NTDXYG/BashExplainer\",\"abstract\":\"$49\",\"description\":\"A novel retrieval-augmented Bash code comment generation method BASHEXPLAINER based on fine-tuned CodeBERT.\",\"selected\":false,\"bibtex\":\"$4a\"},{\"id\":\"DBLP:conf/icbase/TianWY22\",\"title\":\"BUG-T5: A Transformer-based Automatic Title Generation Method for Bug Reports\",\"abbr\":\"ICBASE'22\",\"authors\":[{\"name\":\"Xinyi Tian\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jingkun Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"10\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\"],\"keywords\":[],\"researchArea\":\"transformer-architectures\",\"journal\":\"\",\"conference\":\"Proceedings of the 3rd International Conference on Big Data \u0026 Artificial Intelligence \u0026 Software Engineering\",\"volume\":\"3304\",\"pages\":\"45--50\",\"url\":\"https://ceur-ws.org/Vol-3304/paper06.pdf\",\"abstract\":\"In Github, developers may not clarify and summarize the critical problems in the bug report titles due to a lack of domain knowledge or poor writing skills. Therefore, it is essential to help practitioners draft high-quality titles. In this study, we propose the BUG-T5 method automatically generating titles by fine-tuning the T5 model. In our empirical analysis, we choose a publicly available corpus from Github. After comparing BUG-T5 with four state-ofthe-art baselines (i.e., TextRank, NMT, Transformer, and iTAPE) on ROUGE metrics, we demonstrate the competitiveness of our proposed method, BUG-T5.\",\"description\":\"A novel Transformer-based automatic title generation method for bug reports.\",\"selected\":false,\"bibtex\":\"$4b\"},{\"id\":\"10.1145/3545258.3545260\",\"title\":\"EL-CodeBert: Better Exploiting CodeBert to Support Source Code-Related Classification Tasks\",\"abbr\":\"Internetware'22\",\"authors\":[{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yanlin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"9\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\",\"CCF-C\"],\"keywords\":[\"Source code-related task\",\"Pre-trained model\",\"Fine-tuning\",\"CodeBert\"],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 13th Asia-Pacific Symposium on Internetware\",\"pages\":\"147â€“155\",\"doi\":\"10.1145/3545258.3545260\",\"url\":\"https://doi.org/10.1145/3545258.3545260\",\"code\":\"https://github.com/NTDXYG/EL-CodeBert\",\"abstract\":\"$4c\",\"description\":\"A novel approach EL-CodeBert to better exploit CodeBert for source code-related classification tasks.\",\"selected\":false,\"bibtex\":\"$4d\"},{\"id\":\"9825881\",\"title\":\"SOTitle: A Transformer-based Post Title Generation Approach for Stack Overflow\",\"abbr\":\"SANER'22\",\"authors\":[{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"3\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\",\"CCF-B\"],\"keywords\":[\"Deep learning;Computer languages;Codes;Conferences;Transformers;Software;Graph neural networks;Post title generation;Question post quality;Stack Overflow mining;Transformer;Code snippet;Problem description\"],\"researchArea\":\"neural-networks\",\"journal\":\"\",\"conference\":\"2022 IEEE International Conference on Software Analysis, Evolution and Reengineering\",\"volume\":\"\",\"issue\":\"\",\"pages\":\"577-588\",\"doi\":\"10.1109/SANER53432.2022.00075\",\"code\":\"https://github.com/NTDXYG/SOTitle\",\"abstract\":\"$4e\",\"description\":\"A novel Transformer-based post title generation approach SOTitle for Stack Overflow.\",\"selected\":false,\"bibtex\":\"$4f\"},{\"id\":\"9825869\",\"title\":\"DualSC: Automatic Generation and Summarization of Shellcode via Transformer and Dual Learning\",\"abbr\":\"SANER'22\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yanlin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"3\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\",\"CCF-B\"],\"keywords\":[\"Codes;Design methodology;Conferences;Training data;Information security;Maintenance engineering;Transformers;Program comprehension;Shellcode generation;Shellcode summarization;Shallow Transformer;Dual learning\"],\"researchArea\":\"transformer-architectures\",\"journal\":\"\",\"conference\":\"2022 IEEE International Conference on Software Analysis, Evolution and Reengineering\",\"volume\":\"\",\"issue\":\"\",\"pages\":\"361-372\",\"doi\":\"10.1109/SANER53432.2022.00052\",\"code\":\"https://github.com/NTDXYG/DualSC\",\"abstract\":\"$50\",\"description\":\"A novel approach DualSC to solve the automatic shellcode generation and summarization tasks.\",\"selected\":false,\"bibtex\":\"$51\"},{\"id\":\"YANG2022107858\",\"title\":\"CCGIR: Information retrieval-based code comment generation method for smart contracts\",\"abbr\":\"KBS'22\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yanlin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hao Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"2\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-C\"],\"keywords\":[\"Code comment generation\",\"Smart contract\",\"Information retrieval\",\"Empirical study\",\"Human study\"],\"researchArea\":\"machine-learning\",\"journal\":\"Knowledge-Based Systems\",\"conference\":\"\",\"volume\":\"237\",\"pages\":\"107858\",\"doi\":\"https://doi.org/10.1016/j.knosys.2021.107858\",\"url\":\"https://www.sciencedirect.com/science/article/pii/S0950705121010406\",\"code\":\"https://github.com/NTDXYG/CCGIR\",\"abstract\":\"$52\",\"description\":\"A novel information retrieval-based code comment generation method CCGIR for smart contracts.\",\"selected\":false,\"bibtex\":\"$53\"},{\"id\":\"9712127\",\"title\":\"Fine-grained Pseudo-code Generation Method via Code Feature Extraction and Transformer\",\"abbr\":\"APSEC'21\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yanlin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\",\"CCF-C\"],\"keywords\":[\"Codes;Semantics;Natural languages;Writing;Feature extraction;Transformers;Generators;Program Comprehension;Pseudo-code generation;Deep learning;Transformer;Code feature extraction\"],\"researchArea\":\"transformer-architectures\",\"journal\":\"\",\"conference\":\"Proceedings of the 28th Asia-Pacific Software Engineering Conference\",\"pages\":\"213-222\",\"doi\":\"10.1109/APSEC53868.2021.00029\",\"code\":\"https://github.com/NTDXYG/DeepPseudo\",\"abstract\":\"$54\",\"description\":\"A novel deep pseudo-code generation method DeepPseudo via code feature extraction and Transformer.\",\"selected\":false,\"bibtex\":\"$55\"},{\"id\":\"9623014\",\"title\":\"EKD-BSP: Bug Report Severity Prediction by Extracting Keywords from Description\",\"abbr\":\"DSA'21\",\"authors\":[{\"name\":\"Yanxin Jia\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuyuan Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jinxin Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"month\":\"8\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\"],\"keywords\":[\"Computer bugs;Predictive models;Feature extraction;Open source software;Bug report severity prediction;Bug report description;Keyword extraction;Text mining\"],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 8th International Conference on Dependable Systems and Their Applications\",\"pages\":\"42-53\",\"doi\":\"10.1109/DSA52907.2021.00014\",\"abstract\":\"$56\",\"description\":\"A novel method EKD-BSP to predict the severity of bug reports by extracting keywords from the description.\",\"selected\":false,\"bibtex\":\"$57\"},{\"id\":\"9622968\",\"title\":\"ComFormer: Code Comment Generation via Transformer and Fusion Method-based Hybrid Code Representation\",\"abbr\":\"DSA'21\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jinxin Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuyuan Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhanqi Cui\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"month\":\"8\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\"],\"keywords\":[\"Training;Learning systems;Codes;Tools;Programming;Transformers;Hybrid power systems;Program Comprehension;Code Comment Generation;Hybrid Code Representation;Transformer;Empirical Study\"],\"researchArea\":\"transformer-architectures\",\"journal\":\"\",\"conference\":\"Proceedings of the 8th International Conference on Dependable Systems and Their Applications\",\"pages\":\"30-41\",\"doi\":\"10.1109/DSA52907.2021.00013\",\"code\":\"https://github.com/NTDXYG/ComFormer\",\"abstract\":\"$58\",\"description\":\"A novel method ComFormer to generate code comments via Transformer and fusion method-based hybrid code representation.\",\"selected\":false,\"bibtex\":\"$59\"},{\"id\":\"DBLP:conf/seke/YangZYC21\",\"title\":\"DeepSCC: Source Code Classification Based on Fine-Tuned RoBERTa (S)\",\"abbr\":\"SEKE'21\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yanlin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"EI\",\"CCF-C\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 33rd International Conference on Software Engineering and Knowledge Engineering\",\"pages\":\"499--502\",\"doi\":\"10.18293/SEKE2021-005\",\"url\":\"https://doi.org/10.18293/SEKE2021-005\",\"code\":\"https://github.com/NTDXYG/DeepSCC\",\"abstract\":\"In software engineering-related tasks (such as programming language tag prediction based on code snippets from Stack Overflow), the programming language classification for code snippets is a common task. In this study, we propose a novel method DeepSCC, which uses a fine-tuned RoBERTa model to classify the programming language type of the source code. In our empirical study, we choose a corpus collected from Stack Overflow, which contains 224,445 pairs of code snippets and corresponding language types. After comparing nine state-of-the-art baselines from the fields of source code classification and neural text classification in terms of four performance measures (i.e., Accuracy, Precision, Recall, and F1), we show the competitiveness of our proposed method DeepSCC.\",\"description\":\"A novel source code classification method DeepSCC based on fine-tuned RoBERTa.\",\"selected\":false,\"bibtex\":\"$5a\"},{\"id\":\"Yang2021deep\",\"title\":\"åŸºäºŽæ·±åº¦å­¦ä¹ çš„ Stack Overflow é—®é¢˜å¸–åˆ†ç±»æ–¹æ³•\",\"abbr\":\"å‰æž—å¤§å­¦å­¦æŠ¥ (ç†å­¦ç‰ˆ)'21\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yanxin Jia\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuyuan Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"åŒ—æ ¸\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"å‰æž—å¤§å­¦å­¦æŠ¥ (ç†å­¦ç‰ˆ)\",\"conference\":\"\",\"volume\":\"59\",\"issue\":\"4\",\"pages\":\"922--928\",\"doi\":\"10.13413/j.cnki.jdxblxb.2020165\",\"abstract\":\"The classification methods based on regular expressions and traditional machine learning had the problems of manual extraction of patterns and performance bottleneck, we proposed deep learning-based classification methods for question post, the deep text mining model TextCNN and integrating attention mechanismâ€”TextRNN were used to construct a classification model. The experimental results show that the classification performance of deep learning-based methods is better than the existing benchmark methods on most of the question purpose categories, and the Adam optimizer is better than the SGD optimizer, and the Glove pre-trained word vector is better than randomly generated word vectors. The method classifies posts for the purpose of asking question, which can add a new dimension to the analysis of post discussion topics on Stack Overflow (SO).\",\"description\":\"A deep learning-based classification methods for question post on Stack Overflow.\",\"selected\":false,\"bibtex\":\"$5b\"},{\"id\":\"JSYJ202106040\",\"title\":\"ORESP:åŸºäºŽæœ‰åºå›žå½’çš„è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦é¢„æµ‹æ–¹æ³•\",\"abbr\":\"è®¡ç®—æœºåº”ç”¨ç ”ç©¶'21\",\"authors\":[{\"name\":\"Yanxin Jia\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hao Lu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hao Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"åŒ—æ ¸\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"è®¡ç®—æœºåº”ç”¨ç ”ç©¶\",\"conference\":\"\",\"volume\":\"38\",\"issue\":\"06\",\"pages\":\"1815-1818\",\"doi\":\"10.19734/j.issn.1001-3695.2020.07.0249\",\"abstract\":\"ä¸ºæé«˜è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦çš„é¢„æµ‹æ€§èƒ½ï¼Œé€šè¿‡å……åˆ†è€ƒè™‘è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦æ ‡ç­¾é—´çš„æ¬¡åºæ€§ï¼Œæå‡ºä¸€ç§åŸºäºŽæœ‰åºå›žå½’çš„è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦é¢„æµ‹æ–¹æ³•ORESPã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨åŸºäºŽSpearmançš„ç‰¹å¾é€‰æ‹©æ–¹æ³•æ¥è¯†åˆ«å¹¶ç§»é™¤æ•°æ®é›†å†…çš„å†—ä½™ç‰¹å¾ï¼ŒéšåŽä½¿ç”¨åŸºäºŽæ¯”ä¾‹ä¼˜åŠ¿æ¨¡åž‹çš„ç¥žç»ç½‘ç»œæ¥æž„å»ºé¢„æµ‹æ¨¡åž‹ã€‚é€šè¿‡ä¸Žäº”ç§ç»å…¸åˆ†ç±»æ–¹æ³•çš„æ¯”è¾ƒï¼Œæ‰€æçš„ORESPæ–¹æ³•åœ¨å››ç§ä¸åŒç±»åž‹çš„åº¦é‡ä¸‹å‡å¯å–å¾—æ›´é«˜çš„é¢„æµ‹æ€§èƒ½ï¼Œå…¶ä¸­åŸºäºŽå¹³å‡0-1è¯¯å·®ï¼ˆMZEï¼‰è¯„æµ‹æŒ‡æ ‡ï¼Œé¢„æµ‹æ¨¡åž‹æ€§èƒ½æœ€å¤§å¯æå‡10.3%ï¼›åŸºäºŽå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰è¯„æµ‹æŒ‡æ ‡ï¼Œé¢„æµ‹æ¨¡åž‹æ€§èƒ½æœ€å¤§å¯æå‡12.3%ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå‘çŽ°ä½¿ç”¨åŸºäºŽSpearmançš„ç‰¹å¾é€‰æ‹©æ–¹æ³•å¯ä»¥æœ‰æ•ˆæå‡ORESPæ–¹æ³•çš„é¢„æµ‹æ€§èƒ½ã€‚\",\"description\":\"A novel method ORESP to predict the severity of software defects based on ordered regression.\",\"selected\":false,\"bibtex\":\"@article{JSYJ202106040,\\n  author = {Yanxin Jia and Xiang Chen and Hao Lu and Guang Yang and Hao Lin },\\n  title = {ORESP:åŸºäºŽæœ‰åºå›žå½’çš„è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦é¢„æµ‹æ–¹æ³•},\\n  abbr = {è®¡ç®—æœºåº”ç”¨ç ”ç©¶'21},\\n  tags = {åŒ—æ ¸},\\n  journal = {è®¡ç®—æœºåº”ç”¨ç ”ç©¶},\\n  volume = {38},\\n  number = {06},\\n  pages = {1815-1818},\\n  year = {2021},\\n  issn = {1001-3695},\\n  doi = {10.19734/j.issn.1001-3695.2020.07.0249},\\n  abstract = {ä¸ºæé«˜è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦çš„é¢„æµ‹æ€§èƒ½ï¼Œé€šè¿‡å……åˆ†è€ƒè™‘è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦æ ‡ç­¾é—´çš„æ¬¡åºæ€§ï¼Œæå‡ºä¸€ç§åŸºäºŽæœ‰åºå›žå½’çš„è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦é¢„æµ‹æ–¹æ³•ORESPã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨åŸºäºŽSpearmançš„ç‰¹å¾é€‰æ‹©æ–¹æ³•æ¥è¯†åˆ«å¹¶ç§»é™¤æ•°æ®é›†å†…çš„å†—ä½™ç‰¹å¾ï¼ŒéšåŽä½¿ç”¨åŸºäºŽæ¯”ä¾‹ä¼˜åŠ¿æ¨¡åž‹çš„ç¥žç»ç½‘ç»œæ¥æž„å»ºé¢„æµ‹æ¨¡åž‹ã€‚é€šè¿‡ä¸Žäº”ç§ç»å…¸åˆ†ç±»æ–¹æ³•çš„æ¯”è¾ƒï¼Œæ‰€æçš„ORESPæ–¹æ³•åœ¨å››ç§ä¸åŒç±»åž‹çš„åº¦é‡ä¸‹å‡å¯å–å¾—æ›´é«˜çš„é¢„æµ‹æ€§èƒ½ï¼Œå…¶ä¸­åŸºäºŽå¹³å‡0-1è¯¯å·®ï¼ˆMZEï¼‰è¯„æµ‹æŒ‡æ ‡ï¼Œé¢„æµ‹æ¨¡åž‹æ€§èƒ½æœ€å¤§å¯æå‡10.3%ï¼›åŸºäºŽå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰è¯„æµ‹æŒ‡æ ‡ï¼Œé¢„æµ‹æ¨¡åž‹æ€§èƒ½æœ€å¤§å¯æå‡12.3%ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå‘çŽ°ä½¿ç”¨åŸºäºŽSpearmançš„ç‰¹å¾é€‰æ‹©æ–¹æ³•å¯ä»¥æœ‰æ•ˆæå‡ORESPæ–¹æ³•çš„é¢„æµ‹æ€§èƒ½ã€‚}\\n}\"},{\"id\":\"RJXB202107011\",\"title\":\"ä»£ç æ³¨é‡Šè‡ªåŠ¨ç”Ÿæˆæ–¹æ³•ç»¼è¿°\",\"abbr\":\"è½¯ä»¶å­¦æŠ¥'21\",\"authors\":[{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhanqi Cui\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guozhu Meng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zan Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"EI\",\"ä¸­æ–‡CCF-A\",\"åŒ—æ ¸\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"è½¯ä»¶å­¦æŠ¥\",\"conference\":\"\",\"volume\":\"32\",\"issue\":\"07\",\"pages\":\"2118-2141\",\"doi\":\"10.13328/j.cnki.jos.006258\",\"abstract\":\"During software development and maintenance, code comments often have some problems, such as missing, insufficient, or mismatching with code content. Writing high-quality code comments takes time and effort for developers, and the quality can not be guaranteed, therefore, it is urgent for researchers to design effective automatic code comment generation methods. The automatic code comment generation issue is an active research topic in the program comprehension domain. This study conducts a systematic review of this research topic. The existing methods are divided into three categories:Template-based generation methods, information retrieval-based methods, and deep learning-based methods. Related studies are analyzed and summarizedfor each category. Then, the corpora and comment quality evaluation methods that are often used in previous studiesare analyzed, which can facilitate the experimental study for future studies. Finally, the potential research directions in the future aresummarized and discussed.\",\"description\":\"A review of automatic code comment generation methods.\",\"selected\":false,\"bibtex\":\"$5c\"}]}],false,false]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Publications | Guang Yang\"}],[\"$\",\"meta\",\"1\",{\"name\":\"author\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"2\",{\"name\":\"keywords\",\"content\":\"Guang Yang,PhD,Research,Zhejiang University\"}],[\"$\",\"meta\",\"3\",{\"name\":\"creator\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"4\",{\"name\":\"publisher\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:description\",\"content\":\"Postdoctoral Researcher at Zhejiang University.\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:site_name\",\"content\":\"Guang Yang's Academic Website\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:description\",\"content\":\"Postdoctoral Researcher at Zhejiang University.\"}],[\"$\",\"link\",\"13\",{\"rel\":\"icon\",\"href\":\"/favicon.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>