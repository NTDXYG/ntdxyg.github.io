<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/yg.jpg"/><link rel="stylesheet" href="/_next/static/css/83a1ddabe21917ec.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-af78f5c2c11af041.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-56ea80454b1da280.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-41db43831ab81c7f.js" async=""></script><script src="/_next/static/chunks/82-0a710f0d74b82ba6.js" async=""></script><script src="/_next/static/chunks/748-55ca435efe938ca9.js" async=""></script><script src="/_next/static/chunks/app/page-a86428691349d4e1.js" async=""></script><link rel="icon" href="/favicon.png" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Guang Yang</title><meta name="description" content="Postdoctoral Researcher at Zhejiang University."/><meta name="author" content="Guang Yang"/><meta name="keywords" content="Guang Yang,PhD,Research,Zhejiang University"/><meta name="creator" content="Guang Yang"/><meta name="publisher" content="Guang Yang"/><meta property="og:title" content="Guang Yang"/><meta property="og:description" content="Postdoctoral Researcher at Zhejiang University."/><meta property="og:site_name" content="Guang Yang&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Guang Yang"/><meta name="twitter:description" content="Postdoctoral Researcher at Zhejiang University."/><link rel="icon" href="/favicon.png"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Guang Yang</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-¬´R5pdb¬ª" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Guang Yang" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/yg.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Guang Yang</h1><p class="text-lg text-accent font-medium mb-1">Postdoctoral Researcher</p><p class="text-neutral-600 mb-2">Zhejiang University</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><a href="https://scholar.google.com/citations?user=JFoOXQwAAAAJ&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://orcid.org/my-orcid?orcid=0000-0002-3374-6680" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a><a href="https://github.com/NTDXYG" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="/main.pdf" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="cv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text h-5 w-5" aria-hidden="true"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>Code Generation</div><div>Artificial Intelligence for Software Engineering</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am currently a Postdoctoral Researcher at Zhejiang University, working with <a href="https://xin-xia.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Xin Xia</a> and <a href="https://xing-hu.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Xing Hu</a>.</p>
<p class="mb-4 last:mb-0">Prior to this, I received my B.Sc. and M.Sc. degrees from Nantong University, supervised by <a href="https://smartse.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Xiang Chen</a>.
I obtained my Ph.D. degree from Nanjing University of Aeronautics and Astronautics, supervised by <a href="https://csyuzhou.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Yu Zhou</a> and <a href="https://chentaolue.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Taolue Chen</a>.
From August 2024 to July 2025, I was a visiting scholar at Singapore Management University, advised by <a href="http://www.mysmu.edu/faculty/davidlo/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. David Lo</a>.</p>
<p class="mb-4 last:mb-0">My current research focuses on Code Generation, a trending intersection of Artificial Intelligence and Software Engineering. I have published about 40 papers in total, including 5 first-authored CCF-A papers.</p>
<p class="mb-4 last:mb-0">I am open to research discussions and collaborations. Please feel free to contact me.</p></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All ‚Üí</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Wei Zheng</span>, </span><span><span class="">Xing Hu</span>, </span><span><span class="">Xin Zhou</span>, </span><span><span class="">David Lo</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A novel LLM-as-Judge method for code evaluation, balancing accuracy, efficiency and explainability.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Less is More: DocString Compression in Code Generation</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Wei Cheng</span>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Terry Yue Zhuo</span>, </span><span><span class="">Ke Liu</span>, </span><span><span class="">Xin Zhou</span>, </span><span><span class="">David Lo</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ACM Transactions on Software Engineering and Methodology</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A novel DocString compression method for code generation, achieving significant reduction in token processing cost while preserving the quality of the generated code.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Terry Zhuo</span>, </span><span><span class="">David Lo</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ACM Transactions on Software Engineering and Methodology</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A general and effective loss function DeCE (Deceptive Cross-Entropy) to defend Code Language Models against backdoor attacks, preventing overfitting to backdoor triggers.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Xiangyu Zhang</span>, </span><span><span class="">Terry Yue Zhuo</span>, </span><span><span class="">Taolue Chen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE Transactions on Software Engineering</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A novel approach COTTON to automatically generate CoTs for code generation, achieving significant improvement in performance compared to the state-of-the-art baselines.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">How important are good method names in neural code generation? a model robustness perspective</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Guang Yang</span>, </span><span><span class="">Yu Zhou</span>, </span><span><span class="">Wenhua Yang</span>, </span><span><span class="">Tao Yue</span>, </span><span><span class="">Xiang Chen</span>, </span><span><span class="">Taolue Chen</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ACM Transactions on Software Engineering and Methodology</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A novel approach RADAR to enhance the performance of PCGMs from a model robustness perspective.</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-11</span><p class="text-sm text-neutral-700">Started working as Postdoc at Zhejiang University.</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-10</span><p class="text-sm text-neutral-700">Completed and received my doctoral degree üéâ</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-09</span><p class="text-sm text-neutral-700">Three papers have been accepted by ASE 2025!</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-05</span><p class="text-sm text-neutral-700">One papers have been accepted by ACL Main!</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-05</span><p class="text-sm text-neutral-700">One papers have been accepted by TSE!</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-05</span><p class="text-sm text-neutral-700">Two papers have been accepted by TOSEM!</p></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->November 18, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-af78f5c2c11af041.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-41db43831ab81c7f.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-41db43831ab81c7f.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-41db43831ab81c7f.js\"],\"default\"]\n7:I[4624,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"82\",\"static/chunks/82-0a710f0d74b82ba6.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-55ca435efe938ca9.js\",\"974\",\"static/chunks/app/page-a86428691349d4e1.js\"],\"default\"]\n8:I[9507,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"82\",\"static/chunks/82-0a710f0d74b82ba6.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-55ca435efe938ca9.js\",\"974\",\"static/chunks/app/page-a86428691349d4e1.js\"],\"default\"]\n9:I[5218,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"82\",\"static/chunks/82-0a710f0d74b82ba6.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-55ca435efe938ca9.js\",\"974\",\"static/chunks/app/page-a86428691349d4e1.js\"],\"default\"]\n14:I[1990,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"82\",\"static/chunks/82-0a710f0d74b82ba6.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-55ca435efe938ca9.js\",\"974\",\"static/chunks/app/page-a86428691349d4e1.js\"],\"default\"]\n15:I[9665,[],\"MetadataBoundary\"]\n17:I[9665,[],\"OutletBoundary\"]\n1a:I[4911,[],\"AsyncMetadataOutlet\"]\n1c:I[9665,[],\"ViewportBoundary\"]\n1e:I[6614,[],\"\"]\n:HL[\"/_next/static/css/83a1ddabe21917ec.css\",\"style\"]\na:T736,Trustworthy evaluation methods for code snippets play a crucial role in neural code generation. Traditional methods, which either rel"])</script><script>self.__next_f.push([1,"y on reference solutions or require executable test cases, have inherent limitation in flexibility and scalability. The recent LLM-as-Judge methodology offers a promising alternative by directly evaluating functional consistency between the problem description and the generated code. To systematically understand the landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical study across three diverse datasets. Our investigation reveals the pros and cons of two categories of LLM-as-Judge methods: the methods based on general foundation models can achieve good performance but require complex prompts and lack explainability, while the methods based on reasoning foundation models provide better explainability with simpler prompts but demand substantial computational resources due to their large parameter sizes. To address these limitations, we propose CODE-DITING, a novel code evaluation method that balances accuracy, efficiency and explainability. We develop a data distillation framework that effectively transfers reasoning capabilities from DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing evaluation explainability and reducing the computational cost. With the majority vote strategy in the inference process, CODE-DITING 1.5B outperforms all models with the same magnitude of parameters and achieves performance which would normally exhibit in a model with 5 times of parameter scale. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the parameter volume of these large models. Further experiments show that CODEDITING is robust to preference leakage and can serve as a promising alternative for code evaluation.b:T903,"])</script><script>self.__next_f.push([1,"@inproceedings{yang2025codediting,\n  title = {Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases},\n  abbr = {ASE'25},\n  author = {Yang, Guang and Zhou, Yu and Chen, Xiang and Zheng, Wei and Hu, Xing and Zhou, Xin and Lo, David and Chen, Taolue},\n  tags = {CCF-A;EI},\n  booktitle = {Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering},\n  year = {2025},\n  month = {11},\n  abstract = {Trustworthy evaluation methods for code snippets play a crucial role in neural code generation. Traditional methods, which either rely on reference solutions or require executable test cases, have inherent limitation in flexibility and scalability. The recent LLM-as-Judge methodology offers a promising alternative by directly evaluating functional consistency between the problem description and the generated code. To systematically understand the landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical study across three diverse datasets. Our investigation reveals the pros and cons of two categories of LLM-as-Judge methods: the methods based on general foundation models can achieve good performance but require complex prompts and lack explainability, while the methods based on reasoning foundation models provide better explainability with simpler prompts but demand substantial computational resources due to their large parameter sizes. To address these limitations, we propose CODE-DITING, a novel code evaluation method that balances accuracy, efficiency and explainability. We develop a data distillation framework that effectively transfers reasoning capabilities from DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing evaluation explainability and reducing the computational cost. With the majority vote strategy in the inference process, CODE-DITING 1.5B outperforms all models with the same magnitude of parameters and achieves performance which would normally exhibit in a model with 5 times of parameter scale. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the parameter volume of these large models. Further experiments show that CODEDITING is robust to preference leakage and can serve as a promising alternative for code evaluation.}\n}"])</script><script>self.__next_f.push([1,"c:T55a,The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings, which capture user requirements for the code and are typically used as the prompt for LLMs, often contain redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study show that the state-of-the-art prompt compression methods achieve only about 10% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc, dedicated to DocString compression for code generation. Our experiments on six code generation datasets, five open-source LLMs (1B to 10B parameters) and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25‚Äì40% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this method is to improve efficiency and reduce the token processing cost while maintaining the quality of the generated code, especially when calling third-party APIs.d:T7a9,@article{10.1145/3735636,\n  author = {Yang, Guang and Zhou, Yu and Cheng, Wei and Zhang, Xiangyu and Chen, Xiang and Zhuo, Terry Yue and Liu, Ke and Zhou, Xin and Lo, David and Chen, Taolue},\n  abbr = {TOSEM'25},\n  tags = {SCI-Q1; CCF-A},\n  title = {Less is More: DocString Compression in Code Generation},\n  year = {2025},\n  month = {5},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  issn = {1049-331X},\n  doi = {10.1145/3735636},\n  abstract = {The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code g"])</script><script>self.__next_f.push([1,"eneration, LLMs are used to translate function/method signature and DocString to executable code. DocStrings, which capture user requirements for the code and are typically used as the prompt for LLMs, often contain redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study show that the state-of-the-art prompt compression methods achieve only about 10\\% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc, dedicated to DocString compression for code generation. Our experiments on six code generation datasets, five open-source LLMs (1B to 10B parameters) and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25‚Äì40\\% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this method is to improve efficiency and reduce the token processing cost while maintaining the quality of the generated code, especially when calling third-party APIs.},\n  note = {Just Accepted},\n  journal = {ACM Transactions on Software Engineering and Methodology}\n}e:T856,"])</script><script>self.__next_f.push([1,"Code Language Models (CLMs), particularly those leveraging deep learning, have achieved significant success in code intelligence domain. However, the issue of security, particularly backdoor attacks, is often overlooked in this process. The previous research has focused on designing backdoor attacks for CLMs, but effective defenses have not been adequately addressed. In particular, existing defense methods from natural language processing, when directly applied to CLMs, are not effective enough and lack generality, working well in some models and scenarios but failing in others, thus fall short in consistently mitigating backdoor attacks. To bridge this gap, we first confirm the phenomenon of ‚Äúearly learning‚Äù as a general occurrence during the training of CLMs. This phenomenon refers to that a model initially focuses on the main features of training data but may become more sensitive to backdoor triggers over time, leading to overfitting and susceptibility to backdoor attacks. We then analyze that overfitting to backdoor triggers results from the use of the cross-entropy loss function, where the unboundedness of cross-entropy leads the model to increasingly concentrate on the features of the poisoned data. Based on this insight, we propose a general and effective loss function DeCE (Deceptive Cross-Entropy) by blending deceptive distributions and applying label smoothing to limit the gradient to bounded, which prevents the model from overfitting to backdoor triggers and then enhances the security of CLMs against backdoor attacks. To evaluate the effectiveness of our defense method, we select four code-related tasks as our experiments scenes and conduct experimental analyses on both natural language and two programming languages (Java and Python). Our experiments across multiple models with different sizes (from 125M to 7B) and poisoning ratios demonstrate the applicability and effectiveness of DeCE in enhancing the security of CLMs. The findings emphasize the potential of DeCE as a novel defense mechanism for CLMs, effectively tackling the challenge of securing models against backdoor threats."])</script><script>self.__next_f.push([1,"f:Tac4,"])</script><script>self.__next_f.push([1,"@article{10.1145/3728639,\n  author = {Yang, Guang and Zhou, Yu and Zhang, Xiangyu and Chen, Xiang and Zhuo, Terry and Lo, David and Chen, Taolue},\n  abbr = {TOSEM'25},\n  tags = {SCI-Q1; CCF-A},\n  title = {Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss},\n  year = {2025},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  issn = {1049-331X},\n  url = {https://doi.org/10.1145/3728639},\n  doi = {10.1145/3728639},\n  abstract = {Code Language Models (CLMs), particularly those leveraging deep learning, have achieved significant success in code intelligence domain. However, the issue of security, particularly backdoor attacks, is often overlooked in this process. The previous research has focused on designing backdoor attacks for CLMs, but effective defenses have not been adequately addressed. In particular, existing defense methods from natural language processing, when directly applied to CLMs, are not effective enough and lack generality, working well in some models and scenarios but failing in others, thus fall short in consistently mitigating backdoor attacks. To bridge this gap, we first confirm the phenomenon of ‚Äúearly learning‚Äù as a general occurrence during the training of CLMs. This phenomenon refers to that a model initially focuses on the main features of training data but may become more sensitive to backdoor triggers over time, leading to overfitting and susceptibility to backdoor attacks. We then analyze that overfitting to backdoor triggers results from the use of the cross-entropy loss function, where the unboundedness of cross-entropy leads the model to increasingly concentrate on the features of the poisoned data. Based on this insight, we propose a general and effective loss function DeCE (Deceptive Cross-Entropy) by blending deceptive distributions and applying label smoothing to limit the gradient to bounded, which prevents the model from overfitting to backdoor triggers and then enhances the security of CLMs against backdoor attacks. To evaluate the effectiveness of our defense method, we select four code-related tasks as our experiments scenes and conduct experimental analyses on both natural language and two programming languages (Java and Python). Our experiments across multiple models with different sizes (from 125M to 7B) and poisoning ratios demonstrate the applicability and effectiveness of DeCE in enhancing the security of CLMs. The findings emphasize the potential of DeCE as a novel defense mechanism for CLMs, effectively tackling the challenge of securing models against backdoor threats.},\n  note = {Just Accepted},\n  journal = {ACM Transactions on Software Engineering and Methodology},\n  month = {5}\n}"])</script><script>self.__next_f.push([1,"10:T5cb,Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models ( ‚ÑìLMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most ‚ÑìLMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach COTTON which can leverage ‚ÑìLMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by COTTON boost various ‚ÑìLMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that COTTON not only improves the performance of ‚ÑìLMs, but also enhances the performance of LLMs. Our study showcases the potential of ‚ÑìLMs in software engineering applications.11:T7a3,@ARTICLE{10634302,\n  author = {Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue},\n  abbr = {TSE'24},\n  tags = {SCI-Q1; CCF-A},\n  journal = {IEEE Transactions on Software Engineering},\n  title = {Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models},\n  year = {2024},\n  month = {12},\n  volume = {50},\n  number = {9},\n  pages = {2437-2457},\n  doi = {10.1109/TSE.2024.3440503},\n  abstract = {Large Language Models (LLMs) have demonstrated remarkable potential in code gene"])</script><script>self.__next_f.push([1,"ration. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models ( ‚ÑìLMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most ‚ÑìLMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach COTTON which can leverage ‚ÑìLMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by COTTON boost various ‚ÑìLMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that COTTON not only improves the performance of ‚ÑìLMs, but also enhances the performance of LLMs. Our study showcases the potential of ‚ÑìLMs in software engineering applications.}\n}12:T695,Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neuRAl coDe generAtor Robustifier (RADAR). RADAR consists of two "])</script><script>self.__next_f.push([1,"components: RADAR-Attack and RADAR-Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR-Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR-Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.13:T8a0,"])</script><script>self.__next_f.push([1,"@article{yang2024important,\n  title = {How important are good method names in neural code generation? a model robustness perspective},\n  abbr = {TOSEM'24},\n  tags = {SCI-Q1; CCF-A},\n  month = {3},\n  author = {Yang, Guang and Zhou, Yu and Yang, Wenhua and Yue, Tao and Chen, Xiang and Chen, Taolue},\n  journal = {ACM Transactions on Software Engineering and Methodology},\n  volume = {33},\n  number = {3},\n  pages = {1--35},\n  year = {2024},\n  publisher = {ACM New York, NY, USA},\n  doi = {10.1145/3630010},\n  abstract = {Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neuRAl coDe generAtor Robustifier (RADAR). RADAR consists of two components: RADAR-Attack and RADAR-Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR-Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR-Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.}\n}"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"JKbpbWr2LItkpSsatD5Xy\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/83a1ddabe21917ec.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.png\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"}],\"siteTitle\":\"Guang Yang\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"November 18, 2025\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Guang Yang\",\"title\":\"Postdoctoral Researcher\",\"institution\":\"Zhejiang University\",\"avatar\":\"/yg.jpg\"},\"social\":{\"email\":\"novelyg@outlook.com\",\"google_scholar\":\"https://scholar.google.com/citations?user=JFoOXQwAAAAJ\u0026hl=zh-CN\",\"orcid\":\"https://orcid.org/my-orcid?orcid=0000-0002-3374-6680\",\"github\":\"https://github.com/NTDXYG\",\"cv\":\"/main.pdf\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"Code Generation\",\"Artificial Intelligence for Software Engineering\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$L8\",\"about\",{\"content\":\"I am currently a Postdoctoral Researcher at Zhejiang University, working with [Prof. Xin Xia](https://xin-xia.github.io/) and [Prof. Xing Hu](https://xing-hu.github.io/).\\r\\n\\r\\nPrior to this, I received my B.Sc. and M.Sc. degrees from Nantong University, supervised by [Prof. Xiang Chen](https://smartse.github.io/). \\r\\nI obtained my Ph.D. degree from Nanjing University of Aeronautics and Astronautics, supervised by [Prof. Yu Zhou](https://csyuzhou.github.io/) and [Prof. Taolue Chen](https://chentaolue.github.io/). \\r\\nFrom August 2024 to July 2025, I was a visiting scholar at Singapore Management University, advised by [Prof. David Lo](http://www.mysmu.edu/faculty/davidlo/).\\r\\n\\r\\nMy current research focuses on Code Generation, a trending intersection of Artificial Intelligence and Software Engineering. I have published about 40 papers in total, including 5 first-authored CCF-A papers. \\r\\n\\r\\nI am open to research discussions and collaborations. Please feel free to contact me.\",\"title\":\"About\"}],[\"$\",\"$L9\",\"featured_publications\",{\"publications\":[{\"id\":\"yang2025codediting\",\"title\":\"Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases\",\"abbr\":\"ASE'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wei Zheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xing Hu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Lo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"11\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"CCF-A\",\"EI\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering\",\"code\":\"https://github.com/CODE-DITING/CODE-DITING\",\"abstract\":\"$a\",\"description\":\"A novel LLM-as-Judge method for code evaluation, balancing accuracy, efficiency and explainability.\",\"selected\":true,\"bibtex\":\"$b\"},{\"id\":\"10.1145/3735636\",\"title\":\"Less is More: DocString Compression in Code Generation\",\"abbr\":\"TOSEM'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Wei Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Terry Yue Zhuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Lo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-A\"],\"keywords\":[\"DocString Compression\",\"Code Generation\",\"Large Language Model\"],\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Software Engineering and Methodology\",\"conference\":\"\",\"doi\":\"10.1145/3735636\",\"code\":\"https://github.com/NTDXYG/ShortenDoc\",\"abstract\":\"$c\",\"description\":\"A novel DocString compression method for code generation, achieving significant reduction in token processing cost while preserving the quality of the generated code.\",\"selected\":true,\"bibtex\":\"$d\"},{\"id\":\"10.1145/3728639\",\"title\":\"Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss\",\"abbr\":\"TOSEM'25\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Terry Zhuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Lo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-A\"],\"keywords\":[],\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Software Engineering and Methodology\",\"conference\":\"\",\"doi\":\"10.1145/3728639\",\"url\":\"https://doi.org/10.1145/3728639\",\"code\":\"https://github.com/NTDXYG/DeCE\",\"abstract\":\"$e\",\"description\":\"A general and effective loss function DeCE (Deceptive Cross-Entropy) to defend Code Language Models against backdoor attacks, preventing overfitting to backdoor triggers.\",\"selected\":true,\"bibtex\":\"$f\"},{\"id\":\"10634302\",\"title\":\"Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models\",\"abbr\":\"TSE'24\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Terry Yue Zhuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"12\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-A\"],\"keywords\":[\"Codes;Cotton;Task analysis;Computational modeling;Benchmark testing;Training;Software engineering;Code generation;chain-of-thought;large language model;lightweight language model;program language processing\"],\"researchArea\":\"signal-processing\",\"journal\":\"IEEE Transactions on Software Engineering\",\"conference\":\"\",\"volume\":\"50\",\"issue\":\"9\",\"pages\":\"2437-2457\",\"doi\":\"10.1109/TSE.2024.3440503\",\"code\":\"https://github.com/NTDXYG/COTTON\",\"abstract\":\"$10\",\"description\":\"A novel approach COTTON to automatically generate CoTs for code generation, achieving significant improvement in performance compared to the state-of-the-art baselines.\",\"selected\":true,\"bibtex\":\"$11\"},{\"id\":\"yang2024important\",\"title\":\"How important are good method names in neural code generation? a model robustness perspective\",\"abbr\":\"TOSEM'24\",\"authors\":[{\"name\":\"Guang Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wenhua Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tao Yue\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Taolue Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"3\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"SCI-Q1\",\"CCF-A\"],\"keywords\":[],\"researchArea\":\"neural-networks\",\"journal\":\"ACM Transactions on Software Engineering and Methodology\",\"conference\":\"\",\"volume\":\"33\",\"issue\":\"3\",\"pages\":\"1--35\",\"doi\":\"10.1145/3630010\",\"code\":\"https://github.com/NTDXYG/RADAR\",\"abstract\":\"$12\",\"description\":\"A novel approach RADAR to enhance the performance of PCGMs from a model robustness perspective.\",\"selected\":true,\"bibtex\":\"$13\"}],\"title\":\"Selected Publications\"}],[\"$\",\"$L14\",\"news\",{\"items\":[{\"date\":\"2025-11\",\"content\":\"Started working as Postdoc at Zhejiang University.\"},{\"date\":\"2025-10\",\"content\":\"Completed and received my doctoral degree üéâ\"},{\"date\":\"2025-09\",\"content\":\"Three papers have been accepted by ASE 2025!\"},{\"date\":\"2025-05\",\"content\":\"One papers have been accepted by ACL Main!\"},{\"date\":\"2025-05\",\"content\":\"One papers have been accepted by TSE!\"},{\"date\":\"2025-05\",\"content\":\"Two papers have been accepted by TOSEM!\"}],\"title\":\"News\"}]],false,false,false]}]]}]]}]}],[\"$\",\"$L15\",null,{\"children\":\"$L16\"}],null,[\"$\",\"$L17\",null,{\"children\":[\"$L18\",\"$L19\",[\"$\",\"$L1a\",null,{\"promise\":\"$@1b\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"I20jC94u44gEwzhzS_XLG\",{\"children\":[[\"$\",\"$L1c\",null,{\"children\":\"$L1d\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$1e\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"1f:\"$Sreact.suspense\"\n20:I[4911,[],\"AsyncMetadata\"]\n16:[\"$\",\"$1f\",null,{\"fallback\":null,\"children\":[\"$\",\"$L20\",null,{\"promise\":\"$@21\"}]}]\n"])</script><script>self.__next_f.push([1,"19:null\n"])</script><script>self.__next_f.push([1,"1d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n18:null\n"])</script><script>self.__next_f.push([1,"21:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Guang Yang\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Postdoctoral Researcher at Zhejiang University.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Guang Yang,PhD,Research,Zhejiang University\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Postdoctoral Researcher at Zhejiang University.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Guang Yang's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Guang Yang\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Postdoctoral Researcher at Zhejiang University.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\n1b:{\"metadata\":\"$21:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>